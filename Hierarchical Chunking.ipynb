{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8523982a",
   "metadata": {},
   "source": [
    "# Hierarchical Chunking with Google Gemini\n",
    "## Building Multi-Level Document Understanding Systems\n",
    "\n",
    "This notebook explores hierarchical chunking techniques that create multi-level document representations, preserving both fine-grained details and high-level structure. We'll build intelligent document analysis systems that can reason across different levels of abstraction using Google Gemini.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Understanding hierarchical document structure and multi-level chunking\n",
    "- Implementing document parsing for headers, sections, and subsections\n",
    "- Building tree-based chunk hierarchies with parent-child relationships\n",
    "- Creating adaptive retrieval systems that leverage document structure\n",
    "- Developing context propagation across hierarchy levels\n",
    "- Analyzing performance benefits of hierarchical approaches\n",
    "\n",
    "### Project Overview:\n",
    "We'll create an advanced system that:\n",
    "1. Analyzes document structure to identify hierarchical elements\n",
    "2. Creates multi-level chunks with preserved relationships\n",
    "3. Implements intelligent retrieval across different abstraction levels\n",
    "4. Builds context-aware Q&A that leverages document hierarchy\n",
    "5. Provides comprehensive analysis and visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5dd80",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai sentence-transformers spacy nltk scikit-learn numpy pandas matplotlib seaborn tiktoken networkx anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a150d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional dependencies\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from anytree import Node, RenderTree, PreOrderIter, LevelOrderIter\n",
    "from anytree.exporter import DotExporter\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6738f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API and models\n",
    "GEMINI_API_KEY = \"your-gemini-api-key-here\"  # Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize models\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"✅ All models initialized successfully!\")\n",
    "print(f\"📊 Embedding dimensions: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"🧠 spaCy model: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "print(f\"🌳 Tree structure library: anytree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7835dde",
   "metadata": {},
   "source": [
    "## 2. Understanding Hierarchical Document Structure\n",
    "\n",
    "Hierarchical chunking preserves document structure by creating chunks at multiple levels of granularity, from high-level sections down to individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def demonstrate_hierarchical_concept():\n",
    "    \"\"\"Demonstrate hierarchical document structure concepts.\"\"\"\n",
    "    \n",
    "    sample_document = \"\"\"\n",
    "# Machine Learning Fundamentals\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This field has revolutionized how we approach complex problems across various domains.\n",
    "\n",
    "### 1.1 Key Concepts\n",
    "\n",
    "The core principle of machine learning lies in pattern recognition. Algorithms analyze large datasets to identify patterns and relationships that humans might miss or find too complex to detect manually.\n",
    "\n",
    "### 1.2 Applications\n",
    "\n",
    "Machine learning applications span numerous industries including healthcare, finance, transportation, and entertainment. Each domain presents unique challenges and opportunities for ML implementation.\n",
    "\n",
    "## 2. Types of Learning\n",
    "\n",
    "### 2.1 Supervised Learning\n",
    "\n",
    "Supervised learning uses labeled training data to teach algorithms to predict outcomes. The algorithm learns from input-output pairs and can then make predictions on new, unseen data.\n",
    "\n",
    "Common supervised learning tasks include classification and regression. Classification predicts discrete categories, while regression predicts continuous numerical values.\n",
    "\n",
    "### 2.2 Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds hidden patterns in data without labeled examples. The algorithm must discover structure in the data independently.\n",
    "\n",
    "Clustering and dimensionality reduction are popular unsupervised learning techniques. These methods help understand data structure and relationships.\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    print(\"🌳 Hierarchical Document Structure Demonstration\\n\")\n",
    "    print(\"Sample Document:\")\n",
    "    print(sample_document)\n",
    "    print(f\"\\nTotal tokens: {count_tokens(sample_document)}\")\n",
    "    \n",
    "    # Identify hierarchical elements\n",
    "    lines = sample_document.split('\\n')\n",
    "    hierarchy_levels = {\n",
    "        'Title (Level 0)': [],\n",
    "        'Main Sections (Level 1)': [],\n",
    "        'Subsections (Level 2)': [],\n",
    "        'Content Paragraphs': []\n",
    "    }\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('# ') and not line.startswith('## '):\n",
    "            hierarchy_levels['Title (Level 0)'].append(line[2:].strip())\n",
    "        elif line.startswith('## '):\n",
    "            hierarchy_levels['Main Sections (Level 1)'].append(line[3:].strip())\n",
    "        elif line.startswith('### '):\n",
    "            hierarchy_levels['Subsections (Level 2)'].append(line[4:].strip())\n",
    "        elif line and not line.startswith('#'):\n",
    "            if len(line) > 20:  # Filter out short lines\n",
    "                hierarchy_levels['Content Paragraphs'].append(line[:60] + '...')\n",
    "    \n",
    "    print(f\"\\n🎯 Identified Hierarchical Structure:\")\n",
    "    for level, items in hierarchy_levels.items():\n",
    "        print(f\"\\n{level}:\")\n",
    "        for i, item in enumerate(items[:3], 1):  # Show first 3 items\n",
    "            print(f\"  {i}. {item}\")\n",
    "        if len(items) > 3:\n",
    "            print(f\"  ... and {len(items) - 3} more\")\n",
    "    \n",
    "    print(f\"\\n📊 Hierarchy Benefits:\")\n",
    "    print(f\"  ✅ Preserves document structure and organization\")\n",
    "    print(f\"  ✅ Enables multi-level retrieval (sections vs details)\")\n",
    "    print(f\"  ✅ Maintains context relationships between levels\")\n",
    "    print(f\"  ✅ Supports both broad and specific queries\")\n",
    "    print(f\"  ✅ Facilitates navigation and summarization\")\n",
    "\n",
    "demonstrate_hierarchical_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0103b4",
   "metadata": {},
   "source": [
    "## 3. Implementing Hierarchical Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalChunker:\n",
    "    def __init__(self, max_chunk_size: int = 512, min_chunk_size: int = 50):\n",
    "        \"\"\"\n",
    "        Hierarchical chunker that creates multi-level document representations.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum tokens per chunk\n",
    "            min_chunk_size: Minimum tokens per chunk\n",
    "        \"\"\"\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def _identify_headers(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Identify headers and their hierarchy levels.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        headers = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Markdown-style headers\n",
    "            if line.startswith('#'):\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                title = line.lstrip('#').strip()\n",
    "                headers.append({\n",
    "                    'line_number': i,\n",
    "                    'level': level,\n",
    "                    'title': title,\n",
    "                    'text': line,\n",
    "                    'type': 'markdown_header'\n",
    "                })\n",
    "            \n",
    "            # Numbered headers (1., 1.1, etc.)\n",
    "            elif re.match(r'^\\d+\\.', line) or re.match(r'^\\d+\\.\\d+', line):\n",
    "                parts = line.split('.', 2)\n",
    "                if len(parts) >= 2:\n",
    "                    level = len([p for p in parts[:-1] if p.strip().isdigit()])\n",
    "                    title = parts[-1].strip() if len(parts) > 1 else line\n",
    "                    headers.append({\n",
    "                        'line_number': i,\n",
    "                        'level': level,\n",
    "                        'title': title,\n",
    "                        'text': line,\n",
    "                        'type': 'numbered_header'\n",
    "                    })\n",
    "            \n",
    "            # Pattern-based headers (ALL CAPS, etc.)\n",
    "            elif (line.isupper() and len(line.split()) <= 8 and \n",
    "                  len(line) > 5 and not re.search(r'[.!?]$', line)):\n",
    "                headers.append({\n",
    "                    'line_number': i,\n",
    "                    'level': 1,  # Default level for pattern headers\n",
    "                    'title': line,\n",
    "                    'text': line,\n",
    "                    'type': 'caps_header'\n",
    "                })\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def _extract_sections(self, text: str, headers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract document sections based on headers.\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        \n",
    "        if not headers:\n",
    "            # No headers found, treat entire document as one section\n",
    "            return [{\n",
    "                'id': 0,\n",
    "                'level': 0,\n",
    "                'title': 'Document',\n",
    "                'content': text,\n",
    "                'start_line': 0,\n",
    "                'end_line': len(lines) - 1,\n",
    "                'tokens': self._count_tokens(text),\n",
    "                'parent_id': None\n",
    "            }]\n",
    "        \n",
    "        # Process sections between headers\n",
    "        for i, header in enumerate(headers):\n",
    "            start_line = header['line_number']\n",
    "            \n",
    "            # Find end line (next header or end of document)\n",
    "            if i + 1 < len(headers):\n",
    "                end_line = headers[i + 1]['line_number'] - 1\n",
    "            else:\n",
    "                end_line = len(lines) - 1\n",
    "            \n",
    "            # Extract section content\n",
    "            section_lines = lines[start_line:end_line + 1]\n",
    "            content = '\\n'.join(section_lines).strip()\n",
    "            \n",
    "            if content and self._count_tokens(content) >= self.min_chunk_size:\n",
    "                # Find parent section\n",
    "                parent_id = None\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if headers[j]['level'] < header['level']:\n",
    "                        parent_id = j\n",
    "                        break\n",
    "                \n",
    "                sections.append({\n",
    "                    'id': i,\n",
    "                    'level': header['level'],\n",
    "                    'title': header['title'],\n",
    "                    'content': content,\n",
    "                    'start_line': start_line,\n",
    "                    'end_line': end_line,\n",
    "                    'tokens': self._count_tokens(content),\n",
    "                    'parent_id': parent_id,\n",
    "                    'header_type': header['type']\n",
    "                })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _create_subsection_chunks(self, section: Dict) -> List[Dict]:\n",
    "        \"\"\"Create smaller chunks within a section if needed.\"\"\"\n",
    "        if section['tokens'] <= self.max_chunk_size:\n",
    "            return [section]  # Section is small enough\n",
    "        \n",
    "        # Split section into paragraphs\n",
    "        paragraphs = [p.strip() for p in section['content'].split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            para_tokens = self._count_tokens(paragraph)\n",
    "            \n",
    "            if current_tokens + para_tokens > self.max_chunk_size and current_chunk:\n",
    "                # Create chunk from current paragraphs\n",
    "                chunk_content = '\\n\\n'.join(current_chunk)\n",
    "                chunks.append({\n",
    "                    'id': f\"{section['id']}.{chunk_id}\",\n",
    "                    'level': section['level'] + 1,\n",
    "                    'title': f\"{section['title']} (Part {chunk_id + 1})\",\n",
    "                    'content': chunk_content,\n",
    "                    'tokens': current_tokens,\n",
    "                    'parent_id': section['id'],\n",
    "                    'type': 'subsection_chunk'\n",
    "                })\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk = [paragraph]\n",
    "                current_tokens = para_tokens\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk.append(paragraph)\n",
    "                current_tokens += para_tokens\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk and current_tokens >= self.min_chunk_size:\n",
    "            chunk_content = '\\n\\n'.join(current_chunk)\n",
    "            chunks.append({\n",
    "                'id': f\"{section['id']}.{chunk_id}\",\n",
    "                'level': section['level'] + 1,\n",
    "                'title': f\"{section['title']} (Part {chunk_id + 1})\",\n",
    "                'content': chunk_content,\n",
    "                'tokens': current_tokens,\n",
    "                'parent_id': section['id'],\n",
    "                'type': 'subsection_chunk'\n",
    "            })\n",
    "        \n",
    "        return chunks if chunks else [section]\n",
    "    \n",
    "    def chunk_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create hierarchical chunks from text.\"\"\"\n",
    "        # Clean text\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)  # Normalize multiple newlines\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Identify document structure\n",
    "        headers = self._identify_headers(text)\n",
    "        sections = self._extract_sections(text, headers)\n",
    "        \n",
    "        # Create hierarchical chunks\n",
    "        all_chunks = []\n",
    "        hierarchy_tree = None\n",
    "        \n",
    "        # Create tree structure\n",
    "        nodes = {}\n",
    "        root = Node(\"document\", chunk_id=\"root\", level=-1, title=\"Document Root\")\n",
    "        nodes[\"root\"] = root\n",
    "        \n",
    "        for section in sections:\n",
    "            # Create subsection chunks if needed\n",
    "            section_chunks = self._create_subsection_chunks(section)\n",
    "            \n",
    "            for chunk in section_chunks:\n",
    "                chunk['chunk_type'] = 'hierarchical'\n",
    "                chunk['hierarchy_path'] = self._get_hierarchy_path(chunk, sections)\n",
    "                all_chunks.append(chunk)\n",
    "                \n",
    "                # Add to tree\n",
    "                parent_node = root\n",
    "                if chunk['parent_id'] is not None and str(chunk['parent_id']) in nodes:\n",
    "                    parent_node = nodes[str(chunk['parent_id'])]\n",
    "                \n",
    "                node = Node(\n",
    "                    chunk['title'],\n",
    "                    parent=parent_node,\n",
    "                    chunk_id=str(chunk['id']),\n",
    "                    level=chunk['level'],\n",
    "                    tokens=chunk['tokens'],\n",
    "                    chunk_data=chunk\n",
    "                )\n",
    "                nodes[str(chunk['id'])] = node\n",
    "        \n",
    "        hierarchy_tree = root\n",
    "        \n",
    "        return {\n",
    "            'chunks': all_chunks,\n",
    "            'hierarchy_tree': hierarchy_tree,\n",
    "            'headers': headers,\n",
    "            'sections': sections,\n",
    "            'total_chunks': len(all_chunks),\n",
    "            'max_level': max([c['level'] for c in all_chunks]) if all_chunks else 0,\n",
    "            'total_tokens': sum([c['tokens'] for c in all_chunks])\n",
    "        }\n",
    "    \n",
    "    def _get_hierarchy_path(self, chunk: Dict, sections: List[Dict]) -> List[str]:\n",
    "        \"\"\"Get the hierarchical path for a chunk.\"\"\"\n",
    "        path = [chunk['title']]\n",
    "        current_parent = chunk['parent_id']\n",
    "        \n",
    "        while current_parent is not None:\n",
    "            parent_section = next((s for s in sections if s['id'] == current_parent), None)\n",
    "            if parent_section:\n",
    "                path.insert(0, parent_section['title'])\n",
    "                current_parent = parent_section['parent_id']\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return path\n",
    "\n",
    "print(\"✅ HierarchicalChunker class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b66b7",
   "metadata": {},
   "source": [
    "## 4. Testing Hierarchical Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52605611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test document with clear hierarchical structure\n",
    "hierarchical_test_doc = \"\"\"\n",
    "# Artificial Intelligence: A Comprehensive Guide\n",
    "\n",
    "## 1. Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) represents one of the most significant technological advances of the modern era. It encompasses the development of computer systems that can perform tasks typically requiring human intelligence, such as learning, reasoning, perception, and decision-making.\n",
    "\n",
    "The field of AI has evolved dramatically since its inception in the 1950s. Early pioneers like Alan Turing and John McCarthy laid the groundational work that continues to influence AI development today.\n",
    "\n",
    "### 1.1 Historical Development\n",
    "\n",
    "The history of AI can be traced back to ancient myths and stories of artificial beings endowed with intelligence. However, the modern field of AI began in the mid-20th century with the advent of electronic computers.\n",
    "\n",
    "Key milestones include the development of the first neural networks, the creation of expert systems, and the recent breakthroughs in deep learning and large language models.\n",
    "\n",
    "### 1.2 Current State of AI\n",
    "\n",
    "Today's AI systems demonstrate remarkable capabilities across diverse domains. From natural language processing to computer vision, AI has achieved superhuman performance in many specialized tasks.\n",
    "\n",
    "Modern AI is characterized by machine learning approaches, particularly deep learning, which has enabled significant advances in pattern recognition and decision-making.\n",
    "\n",
    "## 2. Core AI Technologies\n",
    "\n",
    "### 2.1 Machine Learning\n",
    "\n",
    "Machine Learning (ML) is a subset of AI that enables systems to automatically learn and improve from experience without being explicitly programmed. ML algorithms build mathematical models based on training data to make predictions or decisions.\n",
    "\n",
    "There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Each approach has distinct characteristics and applications.\n",
    "\n",
    "#### 2.1.1 Supervised Learning\n",
    "\n",
    "Supervised learning algorithms learn from labeled training data. The algorithm makes predictions based on input-output pairs and is evaluated on its ability to generalize to new, unseen data.\n",
    "\n",
    "Common supervised learning tasks include classification (predicting categories) and regression (predicting continuous values). Popular algorithms include linear regression, decision trees, and support vector machines.\n",
    "\n",
    "#### 2.1.2 Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds patterns in data without labeled examples. These algorithms discover hidden structures in data, such as clusters, associations, or dimensionality reduction.\n",
    "\n",
    "Key unsupervised learning techniques include clustering algorithms like K-means, association rule learning, and principal component analysis for dimensionality reduction.\n",
    "\n",
    "### 2.2 Deep Learning\n",
    "\n",
    "Deep Learning is a specialized subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data.\n",
    "\n",
    "Deep learning has revolutionized many AI applications, particularly in computer vision, natural language processing, and speech recognition. The ability to automatically learn hierarchical representations makes deep learning particularly powerful.\n",
    "\n",
    "#### 2.2.1 Neural Network Architectures\n",
    "\n",
    "Various neural network architectures have been developed for different types of problems. Convolutional Neural Networks (CNNs) excel at image processing, while Recurrent Neural Networks (RNNs) are suited for sequential data.\n",
    "\n",
    "Transformer architectures have become dominant in natural language processing, enabling the development of large language models with unprecedented capabilities.\n",
    "\n",
    "### 2.3 Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) focuses on the interaction between computers and human language. It involves developing systems that can understand, interpret, and generate human language in valuable ways.\n",
    "\n",
    "NLP combines computational linguistics with machine learning to enable computers to process and analyze large amounts of natural language data. Applications include machine translation, sentiment analysis, and chatbots.\n",
    "\n",
    "## 3. AI Applications and Impact\n",
    "\n",
    "### 3.1 Healthcare\n",
    "\n",
    "AI is transforming healthcare through applications in medical imaging, drug discovery, personalized treatment, and clinical decision support. Machine learning algorithms can analyze medical images with accuracy matching or exceeding human specialists.\n",
    "\n",
    "AI-powered systems help identify diseases earlier, predict patient outcomes, and optimize treatment plans. The integration of AI in healthcare promises to improve patient care while reducing costs.\n",
    "\n",
    "### 3.2 Transportation\n",
    "\n",
    "Autonomous vehicles represent one of the most visible applications of AI technology. Self-driving cars use computer vision, sensor fusion, and machine learning to navigate complex environments safely.\n",
    "\n",
    "Beyond autonomous vehicles, AI optimizes traffic flow, improves logistics and supply chain management, and enhances public transportation systems.\n",
    "\n",
    "### 3.3 Finance\n",
    "\n",
    "The financial industry has embraced AI for fraud detection, algorithmic trading, risk assessment, and customer service. Machine learning models can detect fraudulent transactions in real-time and assess credit risk more accurately.\n",
    "\n",
    "AI-powered robo-advisors provide personalized investment advice, while natural language processing enables automated customer support and document analysis.\n",
    "\n",
    "## 4. Challenges and Future Directions\n",
    "\n",
    "### 4.1 Ethical Considerations\n",
    "\n",
    "As AI systems become more powerful and widespread, ethical considerations become increasingly important. Issues include algorithmic bias, privacy concerns, job displacement, and the need for transparent and explainable AI systems.\n",
    "\n",
    "Developing ethical AI requires interdisciplinary collaboration between technologists, ethicists, policymakers, and society at large to ensure AI benefits humanity while minimizing potential harms.\n",
    "\n",
    "### 4.2 Technical Challenges\n",
    "\n",
    "Despite significant progress, AI faces several technical challenges. These include the need for large amounts of training data, computational resource requirements, and the brittleness of AI systems to adversarial examples.\n",
    "\n",
    "Research continues into more efficient algorithms, better generalization capabilities, and AI systems that can learn from fewer examples.\n",
    "\n",
    "### 4.3 Future Prospects\n",
    "\n",
    "The future of AI holds tremendous promise. Anticipated developments include artificial general intelligence (AGI), quantum machine learning, and AI systems that can reason and understand the world more like humans.\n",
    "\n",
    "As AI continues to advance, it will likely transform every aspect of human society, from how we work and learn to how we interact with technology and each other.\n",
    "\"\"\"\n",
    "\n",
    "# Test hierarchical chunking\n",
    "print(\"🌳 Testing Hierarchical Chunking\\n\")\n",
    "\n",
    "hierarchical_chunker = HierarchicalChunker(max_chunk_size=400, min_chunk_size=50)\n",
    "result = hierarchical_chunker.chunk_text(hierarchical_test_doc)\n",
    "\n",
    "print(f\"📊 Hierarchical Chunking Results:\")\n",
    "print(f\"  Total chunks: {result['total_chunks']}\")\n",
    "print(f\"  Maximum hierarchy level: {result['max_level']}\")\n",
    "print(f\"  Total tokens: {result['total_tokens']}\")\n",
    "print(f\"  Headers identified: {len(result['headers'])}\")\n",
    "print(f\"  Sections created: {len(result['sections'])}\")\n",
    "\n",
    "# Display hierarchy structure\n",
    "print(f\"\\n🌲 Document Hierarchy Tree:\")\n",
    "for pre, _, node in RenderTree(result['hierarchy_tree']):\n",
    "    if hasattr(node, 'chunk_data'):\n",
    "        chunk = node.chunk_data\n",
    "        print(f\"{pre}{node.name} (Level {node.level}, {node.tokens} tokens)\")\n",
    "    else:\n",
    "        print(f\"{pre}{node.name}\")\n",
    "\n",
    "# Show chunk details\n",
    "print(f\"\\n📋 Sample Chunk Details:\")\n",
    "for i, chunk in enumerate(result['chunks'][:5]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ID: {chunk['id']}\")\n",
    "    print(f\"  Level: {chunk['level']}\")\n",
    "    print(f\"  Title: {chunk['title']}\")\n",
    "    print(f\"  Tokens: {chunk['tokens']}\")\n",
    "    print(f\"  Hierarchy Path: {' → '.join(chunk['hierarchy_path'])}\")\n",
    "    print(f\"  Content Preview: {chunk['content'][:100]}...\")\n",
    "\n",
    "# Store result for later use\n",
    "hierarchical_result = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b24c58",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hierarchy_structure(result: Dict) -> None:\n",
    "    \"\"\"Visualize the hierarchical structure with multiple views.\"\"\"\n",
    "    \n",
    "    chunks = result['chunks']\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Hierarchical Document Structure Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Chunks by hierarchy level\n",
    "    level_counts = defaultdict(int)\n",
    "    level_tokens = defaultdict(list)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        level = chunk['level']\n",
    "        level_counts[level] += 1\n",
    "        level_tokens[level].append(chunk['tokens'])\n",
    "    \n",
    "    levels = sorted(level_counts.keys())\n",
    "    counts = [level_counts[level] for level in levels]\n",
    "    \n",
    "    ax1.bar([f\"Level {level}\" for level in levels], counts, alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Chunks by Hierarchy Level')\n",
    "    ax1.set_ylabel('Number of Chunks')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Token distribution by level\n",
    "    avg_tokens = [np.mean(level_tokens[level]) for level in levels]\n",
    "    \n",
    "    ax2.bar([f\"Level {level}\" for level in levels], avg_tokens, alpha=0.7, color='lightgreen')\n",
    "    ax2.set_title('Average Tokens per Level')\n",
    "    ax2.set_ylabel('Average Tokens')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Hierarchy depth distribution\n",
    "    depth_distribution = [len(chunk['hierarchy_path']) for chunk in chunks]\n",
    "    \n",
    "    ax3.hist(depth_distribution, bins=max(depth_distribution), alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax3.set_title('Hierarchy Depth Distribution')\n",
    "    ax3.set_xlabel('Hierarchy Depth')\n",
    "    ax3.set_ylabel('Number of Chunks')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Token size distribution\n",
    "    token_sizes = [chunk['tokens'] for chunk in chunks]\n",
    "    \n",
    "    ax4.hist(token_sizes, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax4.axvline(np.mean(token_sizes), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(token_sizes):.1f}')\n",
    "    ax4.set_title('Chunk Token Size Distribution')\n",
    "    ax4.set_xlabel('Tokens')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print hierarchy statistics\n",
    "    print(f\"\\n📊 Hierarchy Statistics:\")\n",
    "    print(f\"  Total levels: {len(levels)}\")\n",
    "    print(f\"  Deepest hierarchy path: {max(depth_distribution)}\")\n",
    "    print(f\"  Average chunk size: {np.mean(token_sizes):.1f} tokens\")\n",
    "    print(f\"  Token size range: {min(token_sizes)} - {max(token_sizes)} tokens\")\n",
    "    \n",
    "    for level in levels:\n",
    "        avg_tokens_level = np.mean(level_tokens[level])\n",
    "        print(f\"  Level {level}: {level_counts[level]} chunks, avg {avg_tokens_level:.1f} tokens\")\n",
    "\n",
    "def create_hierarchy_network(result: Dict) -> None:\n",
    "    \"\"\"Create network visualization of hierarchy structure.\"\"\"\n",
    "    \n",
    "    # Create network graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges based on hierarchy tree\n",
    "    for node in PreOrderIter(result['hierarchy_tree']):\n",
    "        node_id = node.chunk_id if hasattr(node, 'chunk_id') else str(node.name)\n",
    "        \n",
    "        # Add node with attributes\n",
    "        G.add_node(node_id, \n",
    "                  name=str(node.name)[:30] + ('...' if len(str(node.name)) > 30 else ''),\n",
    "                  level=getattr(node, 'level', 0),\n",
    "                  tokens=getattr(node, 'tokens', 0))\n",
    "        \n",
    "        # Add edge to parent\n",
    "        if node.parent:\n",
    "            parent_id = node.parent.chunk_id if hasattr(node.parent, 'chunk_id') else str(node.parent.name)\n",
    "            G.add_edge(parent_id, node_id)\n",
    "    \n",
    "    # Create layout\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Use hierarchical layout\n",
    "    pos = nx.nx_agraph.graphviz_layout(G, prog='dot') if hasattr(nx, 'nx_agraph') else nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    # Color nodes by level\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    \n",
    "    for node_id in G.nodes():\n",
    "        level = G.nodes[node_id]['level']\n",
    "        tokens = G.nodes[node_id]['tokens']\n",
    "        \n",
    "        # Color by level\n",
    "        colors = ['red', 'orange', 'yellow', 'lightgreen', 'lightblue', 'purple']\n",
    "        node_colors.append(colors[min(level, len(colors)-1)])\n",
    "        \n",
    "        # Size by token count\n",
    "        size = max(300, min(1500, tokens * 3)) if tokens > 0 else 300\n",
    "        node_sizes.append(size)\n",
    "    \n",
    "    # Draw network\n",
    "    nx.draw(G, pos, \n",
    "            node_color=node_colors,\n",
    "            node_size=node_sizes,\n",
    "            with_labels=False,\n",
    "            arrows=True,\n",
    "            edge_color='gray',\n",
    "            alpha=0.7,\n",
    "            arrowsize=20)\n",
    "    \n",
    "    # Add labels\n",
    "    node_labels = {node_id: G.nodes[node_id]['name'] for node_id in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, node_labels, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title('Hierarchical Document Structure Network', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Level 0'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Level 1'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', markersize=10, label='Level 2'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Level 3+')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the hierarchical structure\n",
    "print(\"📈 Visualizing Hierarchical Structure\")\n",
    "visualize_hierarchy_structure(hierarchical_result)\n",
    "\n",
    "print(\"\\n🕸️ Creating Hierarchy Network Visualization\")\n",
    "try:\n",
    "    create_hierarchy_network(hierarchical_result)\n",
    "except Exception as e:\n",
    "    print(f\"Network visualization skipped: {e}\")\n",
    "    print(\"Install graphviz for better network layouts: pip install pygraphviz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cd482",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f604440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalQASystem:\n",
    "    def __init__(self, max_chunk_size: int = 400, min_chunk_size: int = 50):\n",
    "        self.chunker = HierarchicalChunker(max_chunk_size, min_chunk_size)\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.hierarchy_result = None\n",
    "        self.chunk_embeddings = None\n",
    "        self.document_title = \"\"\n",
    "        \n",
    "    def load_document(self, text: str, title: str = \"Document\"):\n",
    "        \"\"\"Load document and create hierarchical structure.\"\"\"\n",
    "        self.document_title = title\n",
    "        print(f\"🔄 Processing document with hierarchical chunking...\")\n",
    "        \n",
    "        # Create hierarchical structure\n",
    "        self.hierarchy_result = self.chunker.chunk_text(text)\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        chunk_texts = [chunk['content'] for chunk in self.hierarchy_result['chunks']]\n",
    "        self.chunk_embeddings = self.embedding_model.encode(chunk_texts)\n",
    "        \n",
    "        print(f\"✅ Loaded '{title}' with hierarchical structure\")\n",
    "        print(f\"📊 {self.hierarchy_result['total_chunks']} chunks across {self.hierarchy_result['max_level']+1} levels\")\n",
    "        \n",
    "    def _find_relevant_chunks_by_level(self, question: str, target_level: Optional[int] = None, max_chunks: int = 3) -> List[Dict]:\n",
    "        \"\"\"Find relevant chunks, optionally filtered by hierarchy level.\"\"\"\n",
    "        if not self.hierarchy_result or self.chunk_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        chunks = self.hierarchy_result['chunks']\n",
    "        \n",
    "        # Filter by level if specified\n",
    "        if target_level is not None:\n",
    "            filtered_chunks = [(i, chunk) for i, chunk in enumerate(chunks) if chunk['level'] == target_level]\n",
    "        else:\n",
    "            filtered_chunks = list(enumerate(chunks))\n",
    "        \n",
    "        if not filtered_chunks:\n",
    "            return []\n",
    "        \n",
    "        # Get embeddings for filtered chunks\n",
    "        indices = [i for i, _ in filtered_chunks]\n",
    "        filtered_embeddings = self.chunk_embeddings[indices]\n",
    "        \n",
    "        # Calculate similarities\n",
    "        question_embedding = self.embedding_model.encode([question])\n",
    "        similarities = cosine_similarity(question_embedding, filtered_embeddings)[0]\n",
    "        \n",
    "        # Get top chunks\n",
    "        top_local_indices = np.argsort(similarities)[::-1][:max_chunks]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for local_idx in top_local_indices:\n",
    "            global_idx, chunk = filtered_chunks[local_idx]\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy['similarity_score'] = float(similarities[local_idx])\n",
    "            chunk_copy['global_index'] = global_idx\n",
    "            relevant_chunks.append(chunk_copy)\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def _find_context_from_hierarchy(self, relevant_chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Expand context using hierarchical relationships.\"\"\"\n",
    "        expanded_chunks = relevant_chunks.copy()\n",
    "        chunk_ids = set(str(chunk['id']) for chunk in relevant_chunks)\n",
    "        \n",
    "        # Add parent context for better understanding\n",
    "        for chunk in relevant_chunks:\n",
    "            if chunk['parent_id'] is not None:\n",
    "                parent_id = str(chunk['parent_id'])\n",
    "                if parent_id not in chunk_ids:\n",
    "                    # Find parent chunk\n",
    "                    parent_chunk = next(\n",
    "                        (c for c in self.hierarchy_result['chunks'] if str(c['id']) == parent_id), \n",
    "                        None\n",
    "                    )\n",
    "                    if parent_chunk:\n",
    "                        parent_copy = parent_chunk.copy()\n",
    "                        parent_copy['similarity_score'] = chunk['similarity_score'] * 0.7  # Reduced score\n",
    "                        parent_copy['context_type'] = 'parent_context'\n",
    "                        expanded_chunks.append(parent_copy)\n",
    "                        chunk_ids.add(parent_id)\n",
    "        \n",
    "        # Sort by hierarchy level and document order\n",
    "        expanded_chunks.sort(key=lambda x: (x['level'], x['id']))\n",
    "        \n",
    "        return expanded_chunks\n",
    "    \n",
    "    def answer_question(self, question: str, strategy: str = 'adaptive', max_chunks: int = 3) -> Dict:\n",
    "        \"\"\"Answer question using hierarchical retrieval strategies.\"\"\"\n",
    "        if not self.hierarchy_result:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        print(f\"🔍 Processing question with {strategy} strategy: {question}\")\n",
    "        \n",
    "        if strategy == 'adaptive':\n",
    "            relevant_chunks = self._adaptive_retrieval(question, max_chunks)\n",
    "        elif strategy == 'broad_first':\n",
    "            relevant_chunks = self._broad_first_retrieval(question, max_chunks)\n",
    "        elif strategy == 'detailed_first':\n",
    "            relevant_chunks = self._detailed_first_retrieval(question, max_chunks)\n",
    "        elif strategy == 'multi_level':\n",
    "            relevant_chunks = self._multi_level_retrieval(question, max_chunks)\n",
    "        else:\n",
    "            relevant_chunks = self._find_relevant_chunks_by_level(question, max_chunks=max_chunks)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\"error\": \"No relevant content found\"}\n",
    "        \n",
    "        # Expand context using hierarchy\n",
    "        expanded_chunks = self._find_context_from_hierarchy(relevant_chunks)\n",
    "        \n",
    "        # Prepare context with hierarchical information\n",
    "        context_parts = []\n",
    "        for chunk in expanded_chunks:\n",
    "            hierarchy_path = ' → '.join(chunk['hierarchy_path'])\n",
    "            context_type = chunk.get('context_type', 'primary')\n",
    "            chunk_info = f\"[{hierarchy_path}] ({context_type})\"\n",
    "            context_parts.append(f\"{chunk_info}\\n{chunk['content']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer with hierarchical awareness\n",
    "        answer_prompt = f\"\"\"\n",
    "        You are analyzing a hierarchically structured document \"{self.document_title}\". \n",
    "        Answer the question using the provided context, which includes hierarchical information.\n",
    "        \n",
    "        Context (with hierarchy paths):\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Instructions:\n",
    "        1. Use the hierarchical structure to provide a well-organized answer\n",
    "        2. Reference specific sections when relevant (e.g., \"According to Section 2.1...\")\n",
    "        3. Synthesize information across different hierarchy levels when appropriate\n",
    "        4. If the question requires broad context, emphasize higher-level information\n",
    "        5. If the question requires specific details, focus on lower-level information\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.gemini_model.generate_content(answer_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.text,\n",
    "                \"strategy\": strategy,\n",
    "                \"primary_chunks\": len(relevant_chunks),\n",
    "                \"total_chunks_used\": len(expanded_chunks),\n",
    "                \"chunk_details\": [\n",
    "                    {\n",
    "                        \"id\": chunk['id'],\n",
    "                        \"level\": chunk['level'],\n",
    "                        \"title\": chunk['title'],\n",
    "                        \"similarity\": chunk.get('similarity_score', 0),\n",
    "                        \"hierarchy_path\": ' → '.join(chunk['hierarchy_path']),\n",
    "                        \"context_type\": chunk.get('context_type', 'primary'),\n",
    "                        \"tokens\": chunk['tokens']\n",
    "                    }\n",
    "                    for chunk in expanded_chunks\n",
    "                ],\n",
    "                \"context_tokens\": sum(chunk['tokens'] for chunk in expanded_chunks)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to generate answer: {e}\"}\n",
    "    \n",
    "    def _adaptive_retrieval(self, question: str, max_chunks: int) -> List[Dict]:\n",
    "        \"\"\"Adaptive retrieval based on question characteristics.\"\"\"\n",
    "        # Analyze question to determine appropriate level\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Broad questions typically use higher-level chunks\n",
    "        broad_keywords = ['overview', 'introduction', 'summary', 'what is', 'explain', 'describe']\n",
    "        detailed_keywords = ['how', 'specific', 'detail', 'example', 'step', 'process']\n",
    "        \n",
    "        is_broad = any(keyword in question_lower for keyword in broad_keywords)\n",
    "        is_detailed = any(keyword in question_lower for keyword in detailed_keywords)\n",
    "        \n",
    "        if is_broad and not is_detailed:\n",
    "            # Prefer higher-level chunks\n",
    "            target_levels = [1, 2, 3]  # Prefer levels 1-3\n",
    "        elif is_detailed and not is_broad:\n",
    "            # Prefer lower-level chunks\n",
    "            target_levels = [3, 4, 5]  # Prefer levels 3+\n",
    "        else:\n",
    "            # Mixed or unclear - use all levels\n",
    "            target_levels = None\n",
    "        \n",
    "        if target_levels:\n",
    "            all_relevant = []\n",
    "            for level in target_levels:\n",
    "                level_chunks = self._find_relevant_chunks_by_level(question, level, max_chunks//len(target_levels) + 1)\n",
    "                all_relevant.extend(level_chunks)\n",
    "            \n",
    "            # Sort by similarity and take top chunks\n",
    "            all_relevant.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "            return all_relevant[:max_chunks]\n",
    "        else:\n",
    "            return self._find_relevant_chunks_by_level(question, max_chunks=max_chunks)\n",
    "    \n",
    "    def _broad_first_retrieval(self, question: str, max_chunks: int) -> List[Dict]:\n",
    "        \"\"\"Retrieve higher-level chunks first.\"\"\"\n",
    "        return self._find_relevant_chunks_by_level(question, target_level=1, max_chunks=max_chunks)\n",
    "    \n",
    "    def _detailed_first_retrieval(self, question: str, max_chunks: int) -> List[Dict]:\n",
    "        \"\"\"Retrieve lower-level detailed chunks first.\"\"\"\n",
    "        max_level = self.hierarchy_result['max_level']\n",
    "        return self._find_relevant_chunks_by_level(question, target_level=max_level, max_chunks=max_chunks)\n",
    "    \n",
    "    def _multi_level_retrieval(self, question: str, max_chunks: int) -> List[Dict]:\n",
    "        \"\"\"Retrieve chunks from multiple levels.\"\"\"\n",
    "        all_chunks = []\n",
    "        max_level = self.hierarchy_result['max_level']\n",
    "        \n",
    "        chunks_per_level = max(1, max_chunks // (max_level + 1))\n",
    "        \n",
    "        for level in range(1, max_level + 1):\n",
    "            level_chunks = self._find_relevant_chunks_by_level(question, level, chunks_per_level)\n",
    "            all_chunks.extend(level_chunks)\n",
    "        \n",
    "        # Sort by similarity and return top chunks\n",
    "        all_chunks.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        return all_chunks[:max_chunks]\n",
    "    \n",
    "    def analyze_hierarchy_structure(self) -> Dict:\n",
    "        \"\"\"Analyze the hierarchical structure of the loaded document.\"\"\"\n",
    "        if not self.hierarchy_result:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        chunks = self.hierarchy_result['chunks']\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"hierarchy_levels\": self.hierarchy_result['max_level'] + 1,\n",
    "            \"level_distribution\": defaultdict(int),\n",
    "            \"avg_tokens_per_level\": defaultdict(list),\n",
    "            \"headers_identified\": len(self.hierarchy_result['headers']),\n",
    "            \"sections_created\": len(self.hierarchy_result['sections'])\n",
    "        }\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            level = chunk['level']\n",
    "            analysis['level_distribution'][level] += 1\n",
    "            analysis['avg_tokens_per_level'][level].append(chunk['tokens'])\n",
    "        \n",
    "        # Calculate averages\n",
    "        for level, tokens in analysis['avg_tokens_per_level'].items():\n",
    "            analysis['avg_tokens_per_level'][level] = np.mean(tokens)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"✅ HierarchicalQASystem class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa9421",
   "metadata": {},
   "source": [
    "## 7. Testing Hierarchical Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da440840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hierarchical Q&A system\n",
    "hierarchical_qa = HierarchicalQASystem(max_chunk_size=400, min_chunk_size=50)\n",
    "hierarchical_qa.load_document(hierarchical_test_doc, \"AI Comprehensive Guide\")\n",
    "\n",
    "# Analyze hierarchy structure\n",
    "structure_analysis = hierarchical_qa.analyze_hierarchy_structure()\n",
    "print(f\"\\n📊 Hierarchy Structure Analysis:\")\n",
    "for key, value in structure_analysis.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"    Level {subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test different retrieval strategies\n",
    "test_questions = [\n",
    "    (\"What is artificial intelligence and what are its main components?\", \"adaptive\"),\n",
    "    (\"How do supervised and unsupervised learning differ?\", \"detailed_first\"),\n",
    "    (\"Give me an overview of AI applications across different industries\", \"broad_first\"),\n",
    "    (\"What are the specific neural network architectures mentioned?\", \"multi_level\")\n",
    "]\n",
    "\n",
    "print(\"\\n🧠 Testing Hierarchical Q&A with Different Strategies\\n\")\n",
    "\n",
    "for i, (question, strategy) in enumerate(test_questions[:2], 1):  # Test first 2 questions\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    result = hierarchical_qa.answer_question(question, strategy=strategy, max_chunks=3)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n💡 Answer:\")\n",
    "        print(result[\"answer\"])\n",
    "        \n",
    "        print(f\"\\n📊 Retrieval Details:\")\n",
    "        print(f\"  - Strategy used: {result['strategy']}\")\n",
    "        print(f\"  - Primary chunks: {result['primary_chunks']}\")\n",
    "        print(f\"  - Total chunks used: {result['total_chunks_used']}\")\n",
    "        print(f\"  - Context tokens: {result['context_tokens']}\")\n",
    "        \n",
    "        print(f\"\\n🌳 Hierarchical Context Used:\")\n",
    "        for detail in result['chunk_details']:\n",
    "            context_indicator = \" 📄\" if detail['context_type'] == 'primary' else \" 🔗\"\n",
    "            print(f\"  {context_indicator} {detail['hierarchy_path']}\")\n",
    "            print(f\"    Level {detail['level']}, Similarity: {detail['similarity']:.3f}, {detail['tokens']} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54253c",
   "metadata": {},
   "source": [
    "## 8. Strategy Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hierarchical_strategies(qa_system: HierarchicalQASystem, question: str) -> Dict:\n",
    "    \"\"\"Compare different hierarchical retrieval strategies.\"\"\"\n",
    "    \n",
    "    strategies = ['adaptive', 'broad_first', 'detailed_first', 'multi_level']\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"🔬 Comparing Hierarchical Strategies for: {question}\\n\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"Testing {strategy} strategy...\")\n",
    "        result = qa_system.answer_question(question, strategy=strategy, max_chunks=3)\n",
    "        \n",
    "        if \"error\" not in result:\n",
    "            # Analyze strategy effectiveness\n",
    "            chunk_levels = [detail['level'] for detail in result['chunk_details']]\n",
    "            avg_level = np.mean(chunk_levels)\n",
    "            level_diversity = len(set(chunk_levels))\n",
    "            \n",
    "            results[strategy] = {\n",
    "                'answer_length': len(result['answer']),\n",
    "                'chunks_used': result['total_chunks_used'],\n",
    "                'context_tokens': result['context_tokens'],\n",
    "                'avg_hierarchy_level': avg_level,\n",
    "                'level_diversity': level_diversity,\n",
    "                'chunk_levels': chunk_levels,\n",
    "                'answer_preview': result['answer'][:150] + '...'\n",
    "            }\n",
    "        else:\n",
    "            results[strategy] = {'error': result['error']}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_strategy_comparison(comparison_results: Dict, question: str):\n",
    "    \"\"\"Visualize strategy comparison results.\"\"\"\n",
    "    \n",
    "    strategies = list(comparison_results.keys())\n",
    "    valid_strategies = [s for s in strategies if 'error' not in comparison_results[s]]\n",
    "    \n",
    "    if not valid_strategies:\n",
    "        print(\"No valid results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = {\n",
    "        'answer_length': [comparison_results[s]['answer_length'] for s in valid_strategies],\n",
    "        'chunks_used': [comparison_results[s]['chunks_used'] for s in valid_strategies],\n",
    "        'context_tokens': [comparison_results[s]['context_tokens'] for s in valid_strategies],\n",
    "        'avg_hierarchy_level': [comparison_results[s]['avg_hierarchy_level'] for s in valid_strategies],\n",
    "        'level_diversity': [comparison_results[s]['level_diversity'] for s in valid_strategies]\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Hierarchical Strategy Comparison\\nQuestion: {question[:60]}...', fontsize=14)\n",
    "    \n",
    "    # Answer length\n",
    "    ax1.bar(valid_strategies, metrics['answer_length'], alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Answer Length')\n",
    "    ax1.set_ylabel('Characters')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Chunks used\n",
    "    ax2.bar(valid_strategies, metrics['chunks_used'], alpha=0.7, color='lightgreen')\n",
    "    ax2.set_title('Total Chunks Used')\n",
    "    ax2.set_ylabel('Number of Chunks')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average hierarchy level\n",
    "    ax3.bar(valid_strategies, metrics['avg_hierarchy_level'], alpha=0.7, color='orange')\n",
    "    ax3.set_title('Average Hierarchy Level')\n",
    "    ax3.set_ylabel('Level')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Level diversity\n",
    "    ax4.bar(valid_strategies, metrics['level_diversity'], alpha=0.7, color='lightcoral')\n",
    "    ax4.set_title('Hierarchy Level Diversity')\n",
    "    ax4.set_ylabel('Unique Levels Used')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(f\"\\n📊 Detailed Strategy Comparison:\")\n",
    "    for strategy in valid_strategies:\n",
    "        data = comparison_results[strategy]\n",
    "        print(f\"\\n{strategy.upper()}:\")\n",
    "        print(f\"  Answer length: {data['answer_length']} characters\")\n",
    "        print(f\"  Chunks used: {data['chunks_used']}\")\n",
    "        print(f\"  Context tokens: {data['context_tokens']}\")\n",
    "        print(f\"  Avg hierarchy level: {data['avg_hierarchy_level']:.2f}\")\n",
    "        print(f\"  Level diversity: {data['level_diversity']} unique levels\")\n",
    "        print(f\"  Levels used: {sorted(set(data['chunk_levels']))}\")\n",
    "\n",
    "# Run strategy comparison\n",
    "comparison_question = \"What are the main types of machine learning and their applications?\"\n",
    "strategy_results = compare_hierarchical_strategies(hierarchical_qa, comparison_question)\n",
    "visualize_strategy_comparison(strategy_results, comparison_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42aebcb",
   "metadata": {},
   "source": [
    "## 9. Hierarchical vs. Other Chunking Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f384bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple comparison with other chunking methods\n",
    "def compare_chunking_approaches(text: str, question: str) -> Dict:\n",
    "    \"\"\"Compare hierarchical chunking with other approaches.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Hierarchical chunking (our implementation)\n",
    "    print(\"Testing hierarchical chunking...\")\n",
    "    hierarchical_qa_test = HierarchicalQASystem(max_chunk_size=400)\n",
    "    hierarchical_qa_test.load_document(text, \"Test Document\")\n",
    "    hierarchy_result = hierarchical_qa_test.answer_question(question, strategy='adaptive')\n",
    "    \n",
    "    if \"error\" not in hierarchy_result:\n",
    "        results['Hierarchical'] = {\n",
    "            'chunks': hierarchy_result['total_chunks_used'],\n",
    "            'answer_length': len(hierarchy_result['answer']),\n",
    "            'context_tokens': hierarchy_result['context_tokens'],\n",
    "            'structure_preserved': True,\n",
    "            'hierarchy_levels': len(set(d['level'] for d in hierarchy_result['chunk_details'])),\n",
    "            'method_type': 'Structure-aware'\n",
    "        }\n",
    "    \n",
    "    # 2. Simple fixed-size chunking simulation\n",
    "    print(\"Testing fixed-size chunking...\")\n",
    "    # Simulate fixed-size by breaking text into 400-token chunks\n",
    "    words = text.split()\n",
    "    chunk_size = 100  # words per chunk (roughly 400 tokens)\n",
    "    fixed_chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk_text = ' '.join(words[i:i+chunk_size])\n",
    "        if len(chunk_text.strip()) > 50:\n",
    "            fixed_chunks.append({\n",
    "                'content': chunk_text,\n",
    "                'tokens': count_tokens(chunk_text)\n",
    "            })\n",
    "    \n",
    "    results['Fixed-Size'] = {\n",
    "        'chunks': len(fixed_chunks),\n",
    "        'answer_length': 0,  # Simplified comparison\n",
    "        'context_tokens': sum(chunk['tokens'] for chunk in fixed_chunks[:3]),  # Top 3 chunks\n",
    "        'structure_preserved': False,\n",
    "        'hierarchy_levels': 1,  # Flat structure\n",
    "        'method_type': 'Size-based'\n",
    "    }\n",
    "    \n",
    "    # 3. Paragraph-based chunking simulation\n",
    "    print(\"Testing paragraph-based chunking...\")\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    para_chunks = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if count_tokens(para) >= 50:  # Minimum size\n",
    "            para_chunks.append({\n",
    "                'content': para,\n",
    "                'tokens': count_tokens(para)\n",
    "            })\n",
    "    \n",
    "    results['Paragraph-Based'] = {\n",
    "        'chunks': len(para_chunks),\n",
    "        'answer_length': 0,  # Simplified comparison\n",
    "        'context_tokens': sum(chunk['tokens'] for chunk in para_chunks[:3]),\n",
    "        'structure_preserved': 'Partial',\n",
    "        'hierarchy_levels': 1,\n",
    "        'method_type': 'Content-aware'\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_chunking_comparison(comparison_results: Dict):\n",
    "    \"\"\"Visualize comparison between chunking methods.\"\"\"\n",
    "    \n",
    "    methods = list(comparison_results.keys())\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for method, data in comparison_results.items():\n",
    "        comparison_data.append({\n",
    "            'Method': method,\n",
    "            'Total Chunks': data['chunks'],\n",
    "            'Context Tokens': data['context_tokens'],\n",
    "            'Hierarchy Levels': data['hierarchy_levels'],\n",
    "            'Structure Preserved': data['structure_preserved'],\n",
    "            'Method Type': data['method_type']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n📊 Chunking Method Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize key metrics\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Chunking Method Comparison', fontsize=16)\n",
    "    \n",
    "    # Total chunks\n",
    "    chunk_counts = [comparison_results[m]['chunks'] for m in methods]\n",
    "    ax1.bar(methods, chunk_counts, alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Total Chunks Created')\n",
    "    ax1.set_ylabel('Number of Chunks')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Context tokens\n",
    "    context_tokens = [comparison_results[m]['context_tokens'] for m in methods]\n",
    "    ax2.bar(methods, context_tokens, alpha=0.7, color='lightgreen')\n",
    "    ax2.set_title('Context Tokens (Top 3 Chunks)')\n",
    "    ax2.set_ylabel('Tokens')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hierarchy levels\n",
    "    hierarchy_levels = [comparison_results[m]['hierarchy_levels'] for m in methods]\n",
    "    ax3.bar(methods, hierarchy_levels, alpha=0.7, color='orange')\n",
    "    ax3.set_title('Hierarchy Levels Available')\n",
    "    ax3.set_ylabel('Number of Levels')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comparison\n",
    "comparison_question = \"What are the main AI technologies discussed?\"\n",
    "print(f\"🔬 Comparing Chunking Approaches\\nQuestion: {comparison_question}\\n\")\n",
    "\n",
    "chunking_comparison = compare_chunking_approaches(hierarchical_test_doc, comparison_question)\n",
    "visualize_chunking_comparison(chunking_comparison)\n",
    "\n",
    "print(\"\\n📈 Key Advantages of Hierarchical Chunking:\")\n",
    "print(\"• ✅ Preserves document structure and organization\")\n",
    "print(\"• ✅ Enables multi-level retrieval strategies\")\n",
    "print(\"• ✅ Maintains semantic relationships between sections\")\n",
    "print(\"• ✅ Supports both broad overviews and detailed answers\")\n",
    "print(\"• ✅ Provides context inheritance from parent sections\")\n",
    "print(\"• ✅ Facilitates navigation and understanding of complex documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b66035-5519-468f-ad18-323791873b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
