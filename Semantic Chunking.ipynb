{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Chunking with Google Gemini\n",
    "## Building an Intelligent Document Analysis System\n",
    "\n",
    "This notebook demonstrates advanced semantic chunking techniques that preserve meaning and context while processing large documents. Unlike fixed-size chunking, semantic chunking respects natural language boundaries and document structure.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Understanding semantic chunking principles\n",
    "- Implementing structure-aware chunking strategies\n",
    "- Using embeddings for similarity-based chunking\n",
    "- Building a semantic document Q&A system with Gemini\n",
    "- Comparing semantic vs fixed-size approaches\n",
    "- Advanced techniques for context preservation\n",
    "\n",
    "### Project Overview:\n",
    "We'll build an intelligent system that:\n",
    "1. Analyzes document structure and semantics\n",
    "2. Creates chunks that preserve meaning and context\n",
    "3. Uses embedding similarity for optimal chunk boundaries\n",
    "4. Implements hierarchical chunking strategies\n",
    "5. Provides superior Q&A performance through semantic understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (0.8.5)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.5 MB 647 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: scikit-learn in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (2.2.6)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Requirement already satisfied: matplotlib in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (3.10.3)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: tiktoken in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: google-api-core in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: tqdm in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: pydantic in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: protobuf in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: google-api-python-client in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-generativeai) (2.173.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Requirement already satisfied: scipy in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Requirement already satisfied: Pillow in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 20.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
      "\u001b[K     |████████████████████████████████| 795 kB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: setuptools in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from spacy) (58.1.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: joblib in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Collecting click\n",
      "  Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Downloading blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 254 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Collecting triton==3.3.1\n",
      "  Using cached triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Collecting sympy>=1.13.3\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open<8.0.0,>=5.2.1\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Installing collected packages: mdurl, nvidia-nvjitlink-cu12, markdown-it-py, hf-xet, fsspec, filelock, catalogue, wrapt, srsly, shellingham, rich, nvidia-cusparse-cu12, nvidia-cublas-cu12, murmurhash, mpmath, marisa-trie, huggingface-hub, cymem, click, wasabi, tzdata, typer, triton, tokenizers, sympy, smart-open, safetensors, pytz, preshed, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparselt-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, networkx, language-data, confection, cloudpathlib, blis, weasel, transformers, torch, thinc, spacy-loggers, spacy-legacy, pandas, langcodes, spacy, sentence-transformers, seaborn, nltk\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.13 networkx-3.4.2 nltk-3.9.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pandas-2.3.0 preshed-3.0.10 pytz-2025.2 rich-14.0.0 safetensors-0.5.3 seaborn-0.13.2 sentence-transformers-4.1.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.14.0 thinc-8.3.6 tokenizers-0.21.1 torch-2.7.1 transformers-4.52.4 triton-3.3.1 typer-0.16.0 tzdata-2025.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai sentence-transformers spacy nltk scikit-learn numpy pandas matplotlib seaborn tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohdasimkhan/.pyenv/versions/chunking/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/home/mohdasimkhan/.pyenv/versions/chunking/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mohdasimkhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mohdasimkhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mohdasimkhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download spaCy model and NLTK data\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mohdasimkhan/.pyenv/versions/3.10.2/envs/chunking/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models initialized successfully!\n",
      "📊 Embedding model: 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')  # Replace with your actual API key or configure the key in .env\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize models\n",
    "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"✅ All models initialized successfully!\")\n",
    "print(f\"📊 Embedding model: {embedding_model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Semantic Chunking\n",
    "\n",
    "Semantic chunking preserves the natural structure and meaning of text, unlike fixed-size chunking which can split content arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Semantic Analysis Demo\n",
      "\n",
      "Original text:\n",
      "Machine learning has revolutionized artificial intelligence. It enables computers to learn patterns from data without explicit programming. \n",
      "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can process complex patterns in images, text, and audio.\n",
      "Natural language processing combines linguistics with machine learning. It allows computers to understand and generate human language.\n",
      "\n",
      "Total tokens: 72\n",
      "\n",
      "📝 Sentence breakdown (6 sentences):\n",
      "1. Machine learning has revolutionized artificial intelligence. (9 tokens)\n",
      "2. It enables computers to learn patterns from data without explicit programming. (12 tokens)\n",
      "3. Deep learning, a subset of machine learning, uses neural networks with multiple layers. (16 tokens)\n",
      "4. These networks can process complex patterns in images, text, and audio. (14 tokens)\n",
      "5. Natural language processing combines linguistics with machine learning. (10 tokens)\n",
      "6. It allows computers to understand and generate human language. (10 tokens)\n",
      "\n",
      "🔗 Sentence similarity matrix:\n",
      "1.000  0.383  0.470  0.307  0.508  0.316  \n",
      "0.383  1.000  0.376  0.465  0.319  0.731  \n",
      "0.470  0.376  1.000  0.494  0.330  0.336  \n",
      "0.307  0.465  0.494  1.000  0.286  0.476  \n",
      "0.508  0.319  0.330  0.286  1.000  0.386  \n",
      "0.316  0.731  0.336  0.476  0.386  1.000  \n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Demonstrate semantic vs arbitrary splitting\n",
    "sample_text = \"\"\"\n",
    "Machine learning has revolutionized artificial intelligence. It enables computers to learn patterns from data without explicit programming. \n",
    "Deep learning, a subset of machine learning, uses neural networks with multiple layers. These networks can process complex patterns in images, text, and audio.\n",
    "Natural language processing combines linguistics with machine learning. It allows computers to understand and generate human language.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Semantic Analysis Demo\\n\")\n",
    "print(\"Original text:\")\n",
    "print(sample_text.strip())\n",
    "print(f\"\\nTotal tokens: {count_tokens(sample_text)}\")\n",
    "\n",
    "# Sentence-level analysis\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(f\"\\n📝 Sentence breakdown ({len(sentences)} sentences):\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()} ({count_tokens(sent)} tokens)\")\n",
    "\n",
    "# Semantic similarity between sentences\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"\\n🔗 Sentence similarity matrix:\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        print(f\"{similarity_matrix[i][j]:.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Semantic Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SemanticChunker class implemented!\n"
     ]
    }
   ],
   "source": [
    "class SemanticChunker:\n",
    "    def __init__(self, max_chunk_size: int = 512, min_chunk_size: int = 50, \n",
    "                 similarity_threshold: float = 0.5, strategy: str = 'sentence'):\n",
    "        \"\"\"\n",
    "        Initialize semantic chunker.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum tokens per chunk\n",
    "            min_chunk_size: Minimum tokens per chunk\n",
    "            similarity_threshold: Threshold for semantic similarity\n",
    "            strategy: 'sentence', 'paragraph', 'structure', or 'embedding'\n",
    "        \"\"\"\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.strategy = strategy\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def _extract_sentences(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract sentences with metadata.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.strip()\n",
    "            if sent_text:\n",
    "                sentences.append({\n",
    "                    'text': sent_text,\n",
    "                    'start': sent.start_char,\n",
    "                    'end': sent.end_char,\n",
    "                    'tokens': self._count_tokens(sent_text)\n",
    "                })\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def _extract_paragraphs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract paragraphs as natural chunks.\"\"\"\n",
    "        paragraphs = []\n",
    "        para_texts = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        start_pos = 0\n",
    "        for para_text in para_texts:\n",
    "            end_pos = start_pos + len(para_text)\n",
    "            paragraphs.append({\n",
    "                'text': para_text,\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'tokens': self._count_tokens(para_text)\n",
    "            })\n",
    "            start_pos = end_pos + 2  # Account for \\n\\n\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    def _extract_structural_elements(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract structural elements (headers, sections, etc.).\"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        # Split by multiple newlines first\n",
    "        sections = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        start_pos = 0\n",
    "        for section in sections:\n",
    "            section = section.strip()\n",
    "            if not section:\n",
    "                continue\n",
    "            \n",
    "            # Identify headers (lines that are short and may have numbers/capitals)\n",
    "            lines = section.split('\\n')\n",
    "            is_header = (\n",
    "                len(lines) == 1 and \n",
    "                len(section) < 100 and \n",
    "                (re.match(r'^\\d+\\.', section) or section.isupper() or section.istitle())\n",
    "            )\n",
    "            \n",
    "            element_type = 'header' if is_header else 'content'\n",
    "            \n",
    "            elements.append({\n",
    "                'text': section,\n",
    "                'type': element_type,\n",
    "                'start': start_pos,\n",
    "                'end': start_pos + len(section),\n",
    "                'tokens': self._count_tokens(section)\n",
    "            })\n",
    "            \n",
    "            start_pos += len(section) + 2\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def _chunk_by_embedding_similarity(self, sentences: List[Dict]) -> List[List[Dict]]:\n",
    "        \"\"\"Group sentences by semantic similarity.\"\"\"\n",
    "        if len(sentences) <= 1:\n",
    "            return [sentences]\n",
    "        \n",
    "        # Get embeddings for all sentences\n",
    "        texts = [sent['text'] for sent in sentences]\n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Group similar sentences\n",
    "        chunks = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "            \n",
    "            current_chunk = [sentence]\n",
    "            current_tokens = sentence['tokens']\n",
    "            used_indices.add(i)\n",
    "            \n",
    "            # Find similar sentences to add to this chunk\n",
    "            for j in range(i + 1, len(sentences)):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Check if adding this sentence would exceed token limit\n",
    "                if current_tokens + sentences[j]['tokens'] > self.max_chunk_size:\n",
    "                    break\n",
    "                \n",
    "                # Check semantic similarity\n",
    "                if similarity_matrix[i][j] > self.similarity_threshold:\n",
    "                    current_chunk.append(sentences[j])\n",
    "                    current_tokens += sentences[j]['tokens']\n",
    "                    used_indices.add(j)\n",
    "            \n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Main chunking method based on strategy.\"\"\"\n",
    "        # Clean text\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        if self.strategy == 'sentence':\n",
    "            return self._chunk_by_sentences(text)\n",
    "        elif self.strategy == 'paragraph':\n",
    "            return self._chunk_by_paragraphs(text)\n",
    "        elif self.strategy == 'structure':\n",
    "            return self._chunk_by_structure(text)\n",
    "        elif self.strategy == 'embedding':\n",
    "            return self._chunk_by_embeddings(text)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "    \n",
    "    def _chunk_by_sentences(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk by combining sentences up to token limit.\"\"\"\n",
    "        sentences = self._extract_sentences(text)\n",
    "        chunks = []\n",
    "        \n",
    "        current_chunk_sentences = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Check if adding this sentence would exceed limit\n",
    "            if (current_tokens + sentence['tokens'] > self.max_chunk_size and \n",
    "                current_tokens >= self.min_chunk_size):\n",
    "                \n",
    "                # Save current chunk\n",
    "                chunk_text = ' '.join([s['text'] for s in current_chunk_sentences])\n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'type': 'sentence-based',\n",
    "                    'sentences': current_chunk_sentences.copy(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'sentence_count': len(current_chunk_sentences)\n",
    "                })\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_tokens = sentence['tokens']\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                current_tokens += sentence['tokens']\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_sentences and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join([s['text'] for s in current_chunk_sentences])\n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'type': 'sentence-based',\n",
    "                'sentences': current_chunk_sentences,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_by_paragraphs(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk by paragraphs, combining small ones.\"\"\"\n",
    "        paragraphs = self._extract_paragraphs(text)\n",
    "        chunks = []\n",
    "        \n",
    "        current_chunk_paras = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # If paragraph alone exceeds max size, split it by sentences\n",
    "            if paragraph['tokens'] > self.max_chunk_size:\n",
    "                # Save current chunk first if it exists\n",
    "                if current_chunk_paras:\n",
    "                    chunk_text = '\\n\\n'.join([p['text'] for p in current_chunk_paras])\n",
    "                    chunks.append({\n",
    "                        'id': chunk_id,\n",
    "                        'text': chunk_text,\n",
    "                        'type': 'paragraph-based',\n",
    "                        'paragraphs': current_chunk_paras.copy(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'paragraph_count': len(current_chunk_paras)\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                    current_chunk_paras = []\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Split large paragraph by sentences\n",
    "                sentence_chunks = self._chunk_by_sentences(paragraph['text'])\n",
    "                for sent_chunk in sentence_chunks:\n",
    "                    sent_chunk['id'] = chunk_id\n",
    "                    sent_chunk['type'] = 'paragraph-split'\n",
    "                    chunks.append(sent_chunk)\n",
    "                    chunk_id += 1\n",
    "                \n",
    "            elif (current_tokens + paragraph['tokens'] > self.max_chunk_size and \n",
    "                  current_tokens >= self.min_chunk_size):\n",
    "                \n",
    "                # Save current chunk\n",
    "                chunk_text = '\\n\\n'.join([p['text'] for p in current_chunk_paras])\n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'type': 'paragraph-based',\n",
    "                    'paragraphs': current_chunk_paras.copy(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'paragraph_count': len(current_chunk_paras)\n",
    "                })\n",
    "                \n",
    "                # Start new chunk\n",
    "                current_chunk_paras = [paragraph]\n",
    "                current_tokens = paragraph['tokens']\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk_paras.append(paragraph)\n",
    "                current_tokens += paragraph['tokens']\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_paras and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = '\\n\\n'.join([p['text'] for p in current_chunk_paras])\n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'type': 'paragraph-based',\n",
    "                'paragraphs': current_chunk_paras,\n",
    "                'token_count': current_tokens,\n",
    "                'paragraph_count': len(current_chunk_paras)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_by_structure(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk by document structure (headers, sections).\"\"\"\n",
    "        elements = self._extract_structural_elements(text)\n",
    "        chunks = []\n",
    "        \n",
    "        current_chunk_elements = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        current_header = None\n",
    "        \n",
    "        for element in elements:\n",
    "            if element['type'] == 'header':\n",
    "                # Save previous chunk if it exists\n",
    "                if current_chunk_elements and current_tokens >= self.min_chunk_size:\n",
    "                    chunk_text = '\\n\\n'.join([e['text'] for e in current_chunk_elements])\n",
    "                    chunks.append({\n",
    "                        'id': chunk_id,\n",
    "                        'text': chunk_text,\n",
    "                        'type': 'structure-based',\n",
    "                        'header': current_header,\n",
    "                        'elements': current_chunk_elements.copy(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'element_count': len(current_chunk_elements)\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                \n",
    "                # Start new chunk with header\n",
    "                current_header = element['text']\n",
    "                current_chunk_elements = [element]\n",
    "                current_tokens = element['tokens']\n",
    "                \n",
    "            else:  # content\n",
    "                if (current_tokens + element['tokens'] > self.max_chunk_size and \n",
    "                    current_tokens >= self.min_chunk_size):\n",
    "                    \n",
    "                    # Save current chunk\n",
    "                    chunk_text = '\\n\\n'.join([e['text'] for e in current_chunk_elements])\n",
    "                    chunks.append({\n",
    "                        'id': chunk_id,\n",
    "                        'text': chunk_text,\n",
    "                        'type': 'structure-based',\n",
    "                        'header': current_header,\n",
    "                        'elements': current_chunk_elements.copy(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'element_count': len(current_chunk_elements)\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                    \n",
    "                    # Start new chunk (keep header if it exists)\n",
    "                    if current_header:\n",
    "                        header_element = {'text': current_header, 'type': 'header', \n",
    "                                        'tokens': self._count_tokens(current_header)}\n",
    "                        current_chunk_elements = [header_element, element]\n",
    "                        current_tokens = header_element['tokens'] + element['tokens']\n",
    "                    else:\n",
    "                        current_chunk_elements = [element]\n",
    "                        current_tokens = element['tokens']\n",
    "                else:\n",
    "                    current_chunk_elements.append(element)\n",
    "                    current_tokens += element['tokens']\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_elements and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = '\\n\\n'.join([e['text'] for e in current_chunk_elements])\n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'type': 'structure-based',\n",
    "                'header': current_header,\n",
    "                'elements': current_chunk_elements,\n",
    "                'token_count': current_tokens,\n",
    "                'element_count': len(current_chunk_elements)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_by_embeddings(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Chunk by semantic similarity using embeddings.\"\"\"\n",
    "        sentences = self._extract_sentences(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return self._chunk_by_sentences(text)\n",
    "        \n",
    "        sentence_groups = self._chunk_by_embedding_similarity(sentences)\n",
    "        chunks = []\n",
    "        \n",
    "        for chunk_id, group in enumerate(sentence_groups):\n",
    "            chunk_text = ' '.join([sent['text'] for sent in group])\n",
    "            token_count = sum([sent['tokens'] for sent in group])\n",
    "            \n",
    "            if token_count >= self.min_chunk_size:\n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'type': 'embedding-based',\n",
    "                    'sentences': group,\n",
    "                    'token_count': token_count,\n",
    "                    'sentence_count': len(group),\n",
    "                    'avg_similarity': self._calculate_group_similarity(group)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _calculate_group_similarity(self, sentences: List[Dict]) -> float:\n",
    "        \"\"\"Calculate average similarity within a group of sentences.\"\"\"\n",
    "        if len(sentences) <= 1:\n",
    "            return 1.0\n",
    "        \n",
    "        texts = [sent['text'] for sent in sentences]\n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Calculate average similarity (excluding diagonal)\n",
    "        total_similarity = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(i + 1, len(sentences)):\n",
    "                total_similarity += similarity_matrix[i][j]\n",
    "                count += 1\n",
    "        \n",
    "        return total_similarity / count if count > 0 else 1.0\n",
    "\n",
    "print(\"✅ SemanticChunker class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Different Semantic Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Semantic Chunking Strategies\n",
      "\n",
      "==================================================\n",
      "Strategy: Sentence-based\n",
      "==================================================\n",
      "Number of chunks: 2\n",
      "Average tokens per chunk: 272.5\n",
      "Processing time: 0.071s\n",
      "\n",
      "First chunk preview:\n",
      "Type: sentence-based\n",
      "Tokens: 300\n",
      "Text: 1. Introduction to Machine Learning Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This field ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "Strategy: Paragraph-based\n",
      "==================================================\n",
      "Number of chunks: 2\n",
      "Average tokens per chunk: 272.5\n",
      "Processing time: 0.069s\n",
      "\n",
      "First chunk preview:\n",
      "Type: paragraph-split\n",
      "Tokens: 392\n",
      "Text: 1. Introduction to Machine Learning Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This field ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "Strategy: Structure-based\n",
      "==================================================\n",
      "Number of chunks: 1\n",
      "Average tokens per chunk: 546.0\n",
      "Processing time: 0.000s\n",
      "\n",
      "First chunk preview:\n",
      "Type: structure-based\n",
      "Tokens: 546\n",
      "Text: 1. Introduction to Machine Learning Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This field ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "Strategy: Embedding-based\n",
      "==================================================\n",
      "Number of chunks: 1\n",
      "Average tokens per chunk: 294.0\n",
      "Processing time: 0.157s\n",
      "\n",
      "First chunk preview:\n",
      "Type: embedding-based\n",
      "Tokens: 294\n",
      "Text: 1. Introduction to Machine Learning Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. The core pr...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive test document with clear structure\n",
    "test_document = \"\"\"\n",
    "1. Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. This field has revolutionized how we approach complex problems across various domains.\n",
    "\n",
    "The core principle of machine learning lies in pattern recognition. Algorithms analyze large datasets to identify patterns and relationships that humans might miss or find too complex to detect manually.\n",
    "\n",
    "2. Types of Machine Learning\n",
    "\n",
    "2.1 Supervised Learning\n",
    "\n",
    "Supervised learning uses labeled training data to teach algorithms to predict outcomes. The algorithm learns from input-output pairs and can then make predictions on new, unseen data.\n",
    "\n",
    "Common supervised learning tasks include classification and regression. Classification predicts discrete categories, while regression predicts continuous numerical values.\n",
    "\n",
    "2.2 Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds hidden patterns in data without labeled examples. The algorithm must discover structure in the data independently.\n",
    "\n",
    "Clustering and dimensionality reduction are popular unsupervised learning techniques. Clustering groups similar data points together, while dimensionality reduction simplifies data while preserving important information.\n",
    "\n",
    "2.3 Reinforcement Learning\n",
    "\n",
    "Reinforcement learning trains agents to make decisions through trial and error. The agent receives rewards or penalties based on its actions and learns to maximize long-term rewards.\n",
    "\n",
    "This approach has achieved remarkable success in game playing, robotics, and autonomous systems. The agent learns optimal strategies through exploration and exploitation of its environment.\n",
    "\n",
    "3. Deep Learning Revolution\n",
    "\n",
    "Deep learning represents a paradigm shift in machine learning. These neural networks with multiple hidden layers can learn complex representations automatically.\n",
    "\n",
    "Convolutional neural networks excel at image processing tasks. They can identify features at different levels of abstraction, from edges and textures to complex objects and scenes.\n",
    "\n",
    "Recurrent neural networks handle sequential data effectively. They maintain memory of previous inputs, making them ideal for natural language processing and time series analysis.\n",
    "\n",
    "4. Applications and Impact\n",
    "\n",
    "Machine learning applications span numerous industries. Healthcare benefits from diagnostic assistance and drug discovery. Finance uses ML for fraud detection and algorithmic trading.\n",
    "\n",
    "Autonomous vehicles rely heavily on machine learning for perception and decision-making. These systems must process sensor data in real-time to navigate safely through complex environments.\n",
    "\n",
    "Natural language processing enables machines to understand and generate human language. This technology powers chatbots, translation services, and content generation tools.\n",
    "\n",
    "5. Challenges and Future Directions\n",
    "\n",
    "Despite significant progress, machine learning faces important challenges. Data quality and availability remain critical issues. Biased datasets can lead to unfair or discriminatory outcomes.\n",
    "\n",
    "Explainability is crucial for high-stakes applications. Black-box models make it difficult to understand how decisions are made, limiting trust and adoption in critical domains.\n",
    "\n",
    "The future of machine learning includes automated machine learning, federated learning, and quantum machine learning. These advances promise to make ML more accessible and powerful.\n",
    "\"\"\"\n",
    "\n",
    "# Test all semantic chunking strategies\n",
    "strategies = {\n",
    "    'Sentence-based': SemanticChunker(max_chunk_size=300, strategy='sentence'),\n",
    "    'Paragraph-based': SemanticChunker(max_chunk_size=400, strategy='paragraph'),\n",
    "    'Structure-based': SemanticChunker(max_chunk_size=350, strategy='structure'),\n",
    "    'Embedding-based': SemanticChunker(max_chunk_size=300, similarity_threshold=0.3, strategy='embedding')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"🧪 Testing Semantic Chunking Strategies\\n\")\n",
    "\n",
    "for name, chunker in strategies.items():\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Strategy: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    chunks = chunker.chunk_text(test_document)\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    results[name] = {\n",
    "        'chunks': chunks,\n",
    "        'count': len(chunks),\n",
    "        'avg_tokens': np.mean([c['token_count'] for c in chunks]),\n",
    "        'processing_time': processing_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Average tokens per chunk: {np.mean([c['token_count'] for c in chunks]):.1f}\")\n",
    "    print(f\"Processing time: {processing_time:.3f}s\")\n",
    "    \n",
    "    # Show first chunk as example\n",
    "    if chunks:\n",
    "        print(f\"\\nFirst chunk preview:\")\n",
    "        print(f\"Type: {chunks[0]['type']}\")\n",
    "        print(f\"Tokens: {chunks[0]['token_count']}\")\n",
    "        print(f\"Text: {chunks[0]['text'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Document Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SemanticQASystem class implemented!\n"
     ]
    }
   ],
   "source": [
    "class SemanticQASystem:\n",
    "    def __init__(self, chunking_strategy: str = 'structure', max_chunk_size: int = 400):\n",
    "        self.chunker = SemanticChunker(\n",
    "            max_chunk_size=max_chunk_size, \n",
    "            strategy=chunking_strategy,\n",
    "            similarity_threshold=0.3\n",
    "        )\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.chunks = []\n",
    "        self.chunk_embeddings = None\n",
    "        self.document_title = \"\"\n",
    "        \n",
    "    def load_document(self, text: str, title: str = \"Document\"):\n",
    "        \"\"\"Load and process document with semantic chunking.\"\"\"\n",
    "        self.document_title = title\n",
    "        print(f\"🔄 Processing document with {self.chunker.strategy} chunking...\")\n",
    "        \n",
    "        # Create semantic chunks\n",
    "        self.chunks = self.chunker.chunk_text(text)\n",
    "        \n",
    "        # Generate embeddings for semantic search\n",
    "        chunk_texts = [chunk['text'] for chunk in self.chunks]\n",
    "        self.chunk_embeddings = self.embedding_model.encode(chunk_texts)\n",
    "        \n",
    "        print(f\"✅ Loaded '{title}' with {len(self.chunks)} semantic chunks\")\n",
    "        \n",
    "        # Display chunk type distribution\n",
    "        chunk_types = {}\n",
    "        for chunk in self.chunks:\n",
    "            chunk_type = chunk.get('type', 'unknown')\n",
    "            chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "        \n",
    "        print(f\"📊 Chunk types: {dict(chunk_types)}\")\n",
    "        \n",
    "    def _find_relevant_chunks_semantic(self, question: str, max_chunks: int = 3) -> List[Dict]:\n",
    "        \"\"\"Find relevant chunks using semantic similarity.\"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Get question embedding\n",
    "        question_embedding = self.embedding_model.encode([question])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(question_embedding, self.chunk_embeddings)[0]\n",
    "        \n",
    "        # Get top chunks by similarity\n",
    "        top_indices = np.argsort(similarities)[::-1][:max_chunks]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            chunk['similarity_score'] = float(similarities[idx])\n",
    "            relevant_chunks.append(chunk)\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def _find_relevant_chunks_hybrid(self, question: str, max_chunks: int = 3) -> List[Dict]:\n",
    "        \"\"\"Find relevant chunks using hybrid approach (semantic + keyword).\"\"\"\n",
    "        if not self.chunks:\n",
    "            return []\n",
    "        \n",
    "        # Semantic similarity\n",
    "        semantic_chunks = self._find_relevant_chunks_semantic(question, max_chunks * 2)\n",
    "        \n",
    "        # Keyword matching boost\n",
    "        question_words = set(question.lower().split())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        question_keywords = question_words - stop_words\n",
    "        \n",
    "        for chunk in semantic_chunks:\n",
    "            chunk_words = set(chunk['text'].lower().split())\n",
    "            keyword_overlap = len(question_keywords & chunk_words) / len(question_keywords) if question_keywords else 0\n",
    "            \n",
    "            # Combine semantic and keyword scores\n",
    "            chunk['hybrid_score'] = chunk['similarity_score'] * 0.7 + keyword_overlap * 0.3\n",
    "        \n",
    "        # Sort by hybrid score and return top chunks\n",
    "        semantic_chunks.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return semantic_chunks[:max_chunks]\n",
    "    \n",
    "    def answer_question(self, question: str, use_hybrid: bool = True) -> Dict:\n",
    "        \"\"\"Answer question using semantic understanding.\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        print(f\"🔍 Finding semantically relevant chunks for: {question}\")\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        if use_hybrid:\n",
    "            relevant_chunks = self._find_relevant_chunks_hybrid(question)\n",
    "        else:\n",
    "            relevant_chunks = self._find_relevant_chunks_semantic(question)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\"error\": \"No relevant content found\"}\n",
    "        \n",
    "        # Prepare context with chunk metadata\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(relevant_chunks, 1):\n",
    "            chunk_info = f\"[Chunk {i} - {chunk['type']}]\"\n",
    "            if 'header' in chunk and chunk['header']:\n",
    "                chunk_info += f\" Section: {chunk['header']}\"\n",
    "            context_parts.append(f\"{chunk_info}\\n{chunk['text']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate comprehensive answer\n",
    "        answer_prompt = f\"\"\"\n",
    "        You are an expert document analyst. Based on the semantically relevant context from \"{self.document_title}\", provide a comprehensive and accurate answer.\n",
    "        \n",
    "        Context (from semantically matched sections):\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Instructions:\n",
    "        1. Provide a thorough answer based on the context\n",
    "        2. If information spans multiple sections, synthesize it coherently\n",
    "        3. Mention which sections your answer draws from when relevant\n",
    "        4. If the context doesn't fully address the question, clearly state what's missing\n",
    "        5. Use specific details and examples from the context\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.gemini_model.generate_content(answer_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.text,\n",
    "                \"relevant_chunks\": len(relevant_chunks),\n",
    "                \"chunk_details\": [\n",
    "                    {\n",
    "                        \"id\": chunk['id'],\n",
    "                        \"type\": chunk['type'],\n",
    "                        \"similarity\": chunk.get('similarity_score', 0),\n",
    "                        \"hybrid_score\": chunk.get('hybrid_score', 0),\n",
    "                        \"header\": chunk.get('header', 'N/A')\n",
    "                    }\n",
    "                    for chunk in relevant_chunks\n",
    "                ],\n",
    "                \"context_tokens\": sum(chunk['token_count'] for chunk in relevant_chunks),\n",
    "                \"search_method\": \"hybrid\" if use_hybrid else \"semantic\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to generate answer: {e}\"}\n",
    "    \n",
    "    def analyze_document_structure(self) -> Dict:\n",
    "        \"\"\"Analyze the semantic structure of the loaded document.\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"chunk_types\": {},\n",
    "            \"avg_tokens_per_chunk\": np.mean([c['token_count'] for c in self.chunks]),\n",
    "            \"headers_found\": [],\n",
    "            \"semantic_coherence\": []\n",
    "        }\n",
    "        \n",
    "        # Analyze chunk types\n",
    "        for chunk in self.chunks:\n",
    "            chunk_type = chunk.get('type', 'unknown')\n",
    "            analysis['chunk_types'][chunk_type] = analysis['chunk_types'].get(chunk_type, 0) + 1\n",
    "            \n",
    "            if chunk.get('header'):\n",
    "                analysis['headers_found'].append(chunk['header'])\n",
    "            \n",
    "            if 'avg_similarity' in chunk:\n",
    "                analysis['semantic_coherence'].append(chunk['avg_similarity'])\n",
    "        \n",
    "        if analysis['semantic_coherence']:\n",
    "            analysis['avg_semantic_coherence'] = np.mean(analysis['semantic_coherence'])\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"✅ SemanticQASystem class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading and Testing with Research Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing document with structure chunking...\n",
      "✅ Loaded 'NLP Research Paper' with 1 semantic chunks\n",
      "📊 Chunk types: {'structure-based': 1}\n",
      "\n",
      "📊 Document Structure Analysis:\n",
      "  total_chunks: 1\n",
      "  chunk_types: {'structure-based': 1}\n",
      "  avg_tokens_per_chunk: 1532.0\n",
      "  headers_found: []\n"
     ]
    }
   ],
   "source": [
    "# Extended research document for testing\n",
    "research_document = \"\"\"\n",
    "Advances in Natural Language Processing: From Statistical Methods to Large Language Models\n",
    "\n",
    "Abstract\n",
    "\n",
    "Natural Language Processing (NLP) has undergone dramatic transformation over the past decades. This paper reviews the evolution from statistical methods to modern transformer-based large language models, examining key breakthroughs, current capabilities, and future directions.\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "Natural Language Processing represents the intersection of computational linguistics, artificial intelligence, and computer science. The field aims to enable computers to understand, interpret, and generate human language in ways that are both meaningful and useful.\n",
    "\n",
    "The journey from rule-based systems to statistical methods and eventually to neural networks reflects broader trends in artificial intelligence. Each paradigm shift has brought new capabilities and applications, fundamentally changing how we interact with technology.\n",
    "\n",
    "Modern NLP systems can translate languages, summarize documents, answer questions, and even engage in creative writing. These achievements stem from decades of research and the availability of large-scale computational resources.\n",
    "\n",
    "2. Historical Development\n",
    "\n",
    "2.1 Rule-Based Systems (1950s-1980s)\n",
    "\n",
    "Early NLP systems relied on hand-crafted rules and linguistic knowledge. These systems used grammatical rules, dictionaries, and semantic networks to process language. While limited in scope, they provided important foundations for understanding language structure.\n",
    "\n",
    "Rule-based approaches excelled in narrow domains but struggled with ambiguity and linguistic variation. The complexity of natural language made it impossible to capture all rules manually, leading researchers to explore data-driven approaches.\n",
    "\n",
    "2.2 Statistical Methods (1990s-2000s)\n",
    "\n",
    "The statistical revolution in NLP introduced probabilistic models and machine learning techniques. Hidden Markov Models, Conditional Random Fields, and Support Vector Machines became standard tools for tasks like part-of-speech tagging and named entity recognition.\n",
    "\n",
    "Statistical methods enabled systems to learn from data rather than relying solely on hand-crafted rules. This approach proved more robust to linguistic variation and could handle previously unseen text more effectively.\n",
    "\n",
    "2.3 Neural Networks and Deep Learning (2010s)\n",
    "\n",
    "The introduction of neural networks marked another paradigm shift. Word embeddings like Word2Vec and GloVe provided dense vector representations of words, capturing semantic relationships in continuous space.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks addressed sequential processing challenges. These architectures could maintain context over longer sequences, improving performance on tasks requiring understanding of sentence-level dependencies.\n",
    "\n",
    "3. The Transformer Revolution\n",
    "\n",
    "3.1 Attention Mechanisms\n",
    "\n",
    "The attention mechanism revolutionized sequence modeling by allowing models to focus on relevant parts of the input. Unlike RNNs, attention enables parallel processing and better handling of long-range dependencies.\n",
    "\n",
    "Self-attention mechanisms compute relationships between all positions in a sequence simultaneously. This approach captures complex patterns that sequential models often miss, leading to significant improvements in translation and text understanding tasks.\n",
    "\n",
    "3.2 Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need,\" replaced recurrence with pure attention mechanisms. This design enables efficient parallel training and better scaling to large datasets.\n",
    "\n",
    "Transformers consist of encoder and decoder stacks with multi-head attention and feed-forward networks. The architecture's modularity and effectiveness led to its adoption across numerous NLP tasks.\n",
    "\n",
    "3.3 Pre-training and Transfer Learning\n",
    "\n",
    "BERT introduced bidirectional pre-training, learning representations from both left and right context. This approach significantly improved performance on downstream tasks through transfer learning.\n",
    "\n",
    "GPT models demonstrated the power of autoregressive language modeling for text generation. These models showed that large-scale pre-training on diverse text corpora could produce remarkably fluent and coherent text.\n",
    "\n",
    "4. Large Language Models\n",
    "\n",
    "4.1 Scaling Laws and Emergent Abilities\n",
    "\n",
    "Research has revealed scaling laws governing language model performance. Increasing model size, training data, and computational resources leads to predictable improvements in capabilities.\n",
    "\n",
    "Large models exhibit emergent abilities not present in smaller versions. These include few-shot learning, chain-of-thought reasoning, and the ability to follow complex instructions without explicit training.\n",
    "\n",
    "4.2 Current State-of-the-Art Models\n",
    "\n",
    "Modern large language models like GPT-4, PaLM, and Claude demonstrate remarkable capabilities across diverse tasks. These models can engage in dialogue, solve reasoning problems, write code, and perform creative tasks.\n",
    "\n",
    "The integration of multimodal capabilities allows models to process text, images, and other data types together. This convergence opens new possibilities for AI applications across domains.\n",
    "\n",
    "5. Applications and Impact\n",
    "\n",
    "5.1 Information Retrieval and Question Answering\n",
    "\n",
    "Modern NLP systems excel at information retrieval and question answering. They can understand complex queries, search through vast document collections, and provide accurate, contextual answers.\n",
    "\n",
    "Retrieval-augmented generation combines language models with external knowledge bases. This approach enables systems to access up-to-date information while maintaining the fluency of pre-trained models.\n",
    "\n",
    "5.2 Content Generation and Creative Applications\n",
    "\n",
    "Language models have revolutionized content creation. They can write articles, stories, poetry, and technical documentation with human-like quality. These capabilities are transforming publishing, marketing, and educational content creation.\n",
    "\n",
    "Creative applications include story generation, dialogue systems, and artistic collaboration. AI can now assist human creators in brainstorming, drafting, and refining creative works.\n",
    "\n",
    "5.3 Code Generation and Programming Assistance\n",
    "\n",
    "Code generation represents a breakthrough application of language models. Systems like GitHub Copilot and CodeT5 can generate, complete, and explain code across multiple programming languages.\n",
    "\n",
    "These tools are transforming software development by automating routine coding tasks and helping developers learn new technologies. The integration of AI into development environments is becoming increasingly common.\n",
    "\n",
    "6. Challenges and Limitations\n",
    "\n",
    "6.1 Computational Requirements\n",
    "\n",
    "Training and deploying large language models requires enormous computational resources. The environmental impact and cost of these systems raise concerns about accessibility and sustainability.\n",
    "\n",
    "Efforts to improve efficiency include model compression, distillation, and hardware optimization. However, the tension between model capability and computational efficiency remains a significant challenge.\n",
    "\n",
    "6.2 Bias and Fairness\n",
    "\n",
    "Language models can perpetuate and amplify biases present in training data. These biases can manifest in generated text, affecting fairness and representation across different groups.\n",
    "\n",
    "Addressing bias requires careful dataset curation, bias detection methods, and mitigation strategies. The challenge is balancing bias reduction with maintaining model performance and capabilities.\n",
    "\n",
    "6.3 Hallucination and Factual Accuracy\n",
    "\n",
    "Large language models can generate plausible but incorrect information, a phenomenon known as hallucination. This limitation poses risks for applications requiring factual accuracy.\n",
    "\n",
    "Improving factual accuracy requires better training methods, fact-checking mechanisms, and uncertainty quantification. Research continues to address these fundamental limitations.\n",
    "\n",
    "7. Future Directions\n",
    "\n",
    "7.1 Multimodal Integration\n",
    "\n",
    "The future of NLP involves deeper integration with other modalities. Vision-language models and speech-text systems represent early steps toward more comprehensive AI understanding.\n",
    "\n",
    "Multimodal models can understand images, videos, and audio in context with text. This integration enables richer applications and more natural human-AI interaction.\n",
    "\n",
    "7.2 Reasoning and Planning\n",
    "\n",
    "Enhancing reasoning capabilities remains a key research direction. Current models show promising reasoning abilities but lack the systematic problem-solving approaches of symbolic AI.\n",
    "\n",
    "Combining neural networks with symbolic reasoning could lead to more robust and interpretable AI systems. This hybrid approach might address current limitations in logical reasoning and planning.\n",
    "\n",
    "8. Conclusion\n",
    "\n",
    "Natural Language Processing has evolved from simple rule-based systems to sophisticated neural networks capable of human-like language understanding and generation. The transformer architecture and large-scale pre-training have driven remarkable progress.\n",
    "\n",
    "Current challenges include computational efficiency, bias mitigation, and improving factual accuracy. Future research directions focus on multimodal integration and enhanced reasoning capabilities.\n",
    "\n",
    "As NLP continues to advance, its impact on society will grow. Understanding both the capabilities and limitations of these systems is crucial for responsible development and deployment.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize semantic Q&A system\n",
    "semantic_qa = SemanticQASystem(chunking_strategy='structure', max_chunk_size=400)\n",
    "semantic_qa.load_document(research_document, \"NLP Research Paper\")\n",
    "\n",
    "# Analyze document structure\n",
    "structure_analysis = semantic_qa.analyze_document_structure()\n",
    "print(f\"\\n📊 Document Structure Analysis:\")\n",
    "for key, value in structure_analysis.items():\n",
    "    if key != 'semantic_coherence':  # Skip raw coherence data\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing Semantic Q&A Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test questions targeting different aspects\n",
    "test_questions = [\n",
    "    \"What is the Transformer architecture and why was it revolutionary?\",\n",
    "    \"How did NLP evolve from rule-based systems to neural networks?\",\n",
    "    \"What are the main challenges with large language models?\",\n",
    "    \"Explain the concept of attention mechanisms in neural networks\",\n",
    "    \"What applications has NLP found in code generation?\",\n",
    "    \"What are the future directions for NLP research?\"\n",
    "]\n",
    "\n",
    "print(\"🧠 Testing Semantic Q&A System\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions[:3], 1):  # Test first 3 questions\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Test both semantic and hybrid approaches\n",
    "    result_hybrid = semantic_qa.answer_question(question, use_hybrid=True)\n",
    "    \n",
    "    if \"error\" in result_hybrid:\n",
    "        print(f\"❌ Error: {result_hybrid['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n🎯 Answer (Hybrid Search):\")\n",
    "        print(result_hybrid[\"answer\"])\n",
    "        \n",
    "        print(f\"\\n📈 Search Details:\")\n",
    "        print(f\"  - Method: {result_hybrid['search_method']}\")\n",
    "        print(f\"  - Relevant chunks: {result_hybrid['relevant_chunks']}\")\n",
    "        print(f\"  - Context tokens: {result_hybrid['context_tokens']}\")\n",
    "        \n",
    "        print(f\"\\n📚 Chunk Details:\")\n",
    "        for detail in result_hybrid['chunk_details']:\n",
    "            print(f\"  - Chunk {detail['id']}: {detail['type']} (Similarity: {detail['similarity']:.3f}, Header: {detail['header']})\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Semantic vs Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare semantic chunking with fixed-size chunking\n",
    "from typing import Dict, Any\n",
    "\n",
    "class FixedSizeChunker:\n",
    "    \"\"\"Simple fixed-size chunker for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 400, overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Create fixed-size chunks.\"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            end = min(start + self.chunk_size, len(tokens))\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens)\n",
    "            \n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'type': 'fixed-size',\n",
    "                'token_count': len(chunk_tokens),\n",
    "                'start_token': start,\n",
    "                'end_token': end\n",
    "            })\n",
    "            \n",
    "            start = end - self.overlap\n",
    "            chunk_id += 1\n",
    "            \n",
    "            if end >= len(tokens):\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "def compare_chunking_methods(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Compare different chunking methods.\"\"\"\n",
    "    \n",
    "    # Initialize chunkers\n",
    "    fixed_chunker = FixedSizeChunker(chunk_size=400, overlap=50)\n",
    "    semantic_chunkers = {\n",
    "        'Semantic-Sentence': SemanticChunker(max_chunk_size=400, strategy='sentence'),\n",
    "        'Semantic-Paragraph': SemanticChunker(max_chunk_size=400, strategy='paragraph'),\n",
    "        'Semantic-Structure': SemanticChunker(max_chunk_size=400, strategy='structure'),\n",
    "        'Semantic-Embedding': SemanticChunker(max_chunk_size=400, strategy='embedding')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test fixed-size chunking\n",
    "    start_time = time.time()\n",
    "    fixed_chunks = fixed_chunker.chunk_text(text)\n",
    "    fixed_time = time.time() - start_time\n",
    "    \n",
    "    results['Fixed-Size'] = {\n",
    "        'chunks': fixed_chunks,\n",
    "        'count': len(fixed_chunks),\n",
    "        'avg_tokens': np.mean([c['token_count'] for c in fixed_chunks]),\n",
    "        'std_tokens': np.std([c['token_count'] for c in fixed_chunks]),\n",
    "        'processing_time': fixed_time,\n",
    "        'strategy': 'fixed-size'\n",
    "    }\n",
    "    \n",
    "    # Test semantic chunking methods\n",
    "    for name, chunker in semantic_chunkers.items():\n",
    "        start_time = time.time()\n",
    "        chunks = chunker.chunk_text(text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        results[name] = {\n",
    "            'chunks': chunks,\n",
    "            'count': len(chunks),\n",
    "            'avg_tokens': np.mean([c['token_count'] for c in chunks]),\n",
    "            'std_tokens': np.std([c['token_count'] for c in chunks]),\n",
    "            'processing_time': processing_time,\n",
    "            'strategy': chunker.strategy\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "print(\"⚖️ Comparing Chunking Methods\\n\")\n",
    "comparison_results = compare_chunking_methods(research_document)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': method,\n",
    "        'Chunk Count': data['count'],\n",
    "        'Avg Tokens': f\"{data['avg_tokens']:.1f}\",\n",
    "        'Token Std': f\"{data['std_tokens']:.1f}\",\n",
    "        'Processing Time (s)': f\"{data['processing_time']:.3f}\",\n",
    "        'Strategy': data['strategy']\n",
    "    }\n",
    "    for method, data in comparison_results.items()\n",
    "])\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Chunking Method Comparison', fontsize=16)\n",
    "\n",
    "methods = list(comparison_results.keys())\n",
    "chunk_counts = [comparison_results[m]['count'] for m in methods]\n",
    "avg_tokens = [comparison_results[m]['avg_tokens'] for m in methods]\n",
    "std_tokens = [comparison_results[m]['std_tokens'] for m in methods]\n",
    "proc_times = [comparison_results[m]['processing_time'] for m in methods]\n",
    "\n",
    "# Chunk count comparison\n",
    "ax1.bar(methods, chunk_counts, alpha=0.7)\n",
    "ax1.set_title('Number of Chunks')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average tokens per chunk\n",
    "ax2.bar(methods, avg_tokens, alpha=0.7, color='orange')\n",
    "ax2.set_title('Average Tokens per Chunk')\n",
    "ax2.set_ylabel('Tokens')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Token standard deviation (consistency)\n",
    "ax3.bar(methods, std_tokens, alpha=0.7, color='green')\n",
    "ax3.set_title('Token Count Standard Deviation')\n",
    "ax3.set_ylabel('Std Deviation')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Processing time\n",
    "ax4.bar(methods, proc_times, alpha=0.7, color='red')\n",
    "ax4.set_title('Processing Time')\n",
    "ax4.set_ylabel('Seconds')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Key Insights:\")\n",
    "print(\"• Semantic chunking generally produces fewer, more coherent chunks\")\n",
    "print(\"• Structure-based chunking respects document organization\")\n",
    "print(\"• Embedding-based chunking groups semantically similar content\")\n",
    "print(\"• Fixed-size chunking is fastest but may break semantic boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_semantic_coherence(chunks: List[Dict], embedding_model) -> Dict:\n",
    "    \"\"\"Analyze semantic coherence within and between chunks.\"\"\"\n",
    "    \n",
    "    if len(chunks) < 2:\n",
    "        return {\"error\": \"Need at least 2 chunks for analysis\"}\n",
    "    \n",
    "    # Get embeddings for all chunks\n",
    "    chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = embedding_model.encode(chunk_texts)\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Analyze coherence metrics\n",
    "    coherence_metrics = {\n",
    "        'avg_similarity': np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]),\n",
    "        'similarity_std': np.std(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]),\n",
    "        'max_similarity': np.max(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]),\n",
    "        'min_similarity': np.min(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]),\n",
    "        'similarity_matrix': similarity_matrix\n",
    "    }\n",
    "    \n",
    "    # Find most and least similar chunk pairs\n",
    "    triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "    similarities = similarity_matrix[triu_indices]\n",
    "    \n",
    "    max_idx = np.argmax(similarities)\n",
    "    min_idx = np.argmin(similarities)\n",
    "    \n",
    "    most_similar_pair = (triu_indices[0][max_idx], triu_indices[1][max_idx])\n",
    "    least_similar_pair = (triu_indices[0][min_idx], triu_indices[1][min_idx])\n",
    "    \n",
    "    coherence_metrics['most_similar_chunks'] = {\n",
    "        'indices': most_similar_pair,\n",
    "        'similarity': similarities[max_idx],\n",
    "        'chunk1_preview': chunks[most_similar_pair[0]]['text'][:100] + '...',\n",
    "        'chunk2_preview': chunks[most_similar_pair[1]]['text'][:100] + '...'\n",
    "    }\n",
    "    \n",
    "    coherence_metrics['least_similar_chunks'] = {\n",
    "        'indices': least_similar_pair,\n",
    "        'similarity': similarities[min_idx],\n",
    "        'chunk1_preview': chunks[least_similar_pair[0]]['text'][:100] + '...',\n",
    "        'chunk2_preview': chunks[least_similar_pair[1]]['text'][:100] + '...'\n",
    "    }\n",
    "    \n",
    "    return coherence_metrics\n",
    "\n",
    "def visualize_chunk_similarity(chunks: List[Dict], title: str = \"Chunk Similarity Matrix\"):\n",
    "    \"\"\"Visualize semantic similarity between chunks.\"\"\"\n",
    "    \n",
    "    if len(chunks) < 2:\n",
    "        print(\"Need at least 2 chunks for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Get embeddings and similarity matrix\n",
    "    chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = embedding_model.encode(chunk_texts)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create labels with chunk info\n",
    "    labels = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_type = chunk.get('type', 'unknown')\n",
    "        header = chunk.get('header', '')\n",
    "        if header:\n",
    "            label = f\"{i}: {header[:20]}...\"\n",
    "        else:\n",
    "            label = f\"{i}: {chunk_type}\"\n",
    "        labels.append(label)\n",
    "    \n",
    "    sns.heatmap(similarity_matrix, \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='viridis',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze semantic coherence for different chunking strategies\n",
    "print(\"🔬 Advanced Semantic Analysis\\n\")\n",
    "\n",
    "for method_name, result_data in list(comparison_results.items())[:3]:  # Analyze first 3 methods\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing: {method_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    chunks = result_data['chunks']\n",
    "    coherence = analyze_semantic_coherence(chunks, embedding_model)\n",
    "    \n",
    "    if \"error\" not in coherence:\n",
    "        print(f\"Average chunk similarity: {coherence['avg_similarity']:.3f}\")\n",
    "        print(f\"Similarity std deviation: {coherence['similarity_std']:.3f}\")\n",
    "        print(f\"Similarity range: {coherence['min_similarity']:.3f} - {coherence['max_similarity']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nMost similar chunks (similarity: {coherence['most_similar_chunks']['similarity']:.3f}):\")\n",
    "        print(f\"  Chunk {coherence['most_similar_chunks']['indices'][0]}: {coherence['most_similar_chunks']['chunk1_preview']}\")\n",
    "        print(f\"  Chunk {coherence['most_similar_chunks']['indices'][1]}: {coherence['most_similar_chunks']['chunk2_preview']}\")\n",
    "        \n",
    "        # Visualize similarity matrix for structure-based chunking\n",
    "        if 'Structure' in method_name:\n",
    "            visualize_chunk_similarity(chunks, f\"Semantic Similarity: {method_name}\")\n",
    "    \n",
    "    time.sleep(0.5)  # Brief pause between analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Semantic Q&A Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_semantic_qa():\n",
    "    \"\"\"Interactive demo of semantic Q&A system.\"\"\"\n",
    "    print(\"🧠 Interactive Semantic Q&A Demo\")\n",
    "    print(\"Ask questions about the NLP research document!\")\n",
    "    print(\"Features: Semantic chunk retrieval, hybrid search, context awareness\")\n",
    "    print(\"Type 'quit' to exit, 'stats' for document statistics\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"❓ Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"👋 Thank you for using the Semantic Q&A system!\")\n",
    "            break\n",
    "        \n",
    "        if question.lower() == 'stats':\n",
    "            stats = semantic_qa.analyze_document_structure()\n",
    "            print(\"\\n📊 Document Statistics:\")\n",
    "            for key, value in stats.items():\n",
    "                if key != 'semantic_coherence':\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n🔍 Processing semantic query: {question}\")\n",
    "        \n",
    "        # Get both semantic and hybrid results\n",
    "        result_semantic = semantic_qa.answer_question(question, use_hybrid=False)\n",
    "        result_hybrid = semantic_qa.answer_question(question, use_hybrid=True)\n",
    "        \n",
    "        if \"error\" in result_hybrid:\n",
    "            print(f\"❌ Error: {result_hybrid['error']}\")\n",
    "        else:\n",
    "            print(f\"\\n🎯 Answer (Hybrid Semantic Search):\")\n",
    "            print(result_hybrid[\"answer\"])\n",
    "            \n",
    "            print(f\"\\n📊 Search Analysis:\")\n",
    "            print(f\"  - Chunks used: {result_hybrid['relevant_chunks']}\")\n",
    "            print(f\"  - Context tokens: {result_hybrid['context_tokens']}\")\n",
    "            print(f\"  - Search method: {result_hybrid['search_method']}\")\n",
    "            \n",
    "            print(f\"\\n🧩 Relevant Chunks:\")\n",
    "            for detail in result_hybrid['chunk_details']:\n",
    "                print(f\"  • Chunk {detail['id']}: {detail['type']} | Sim: {detail['similarity']:.3f} | Hybrid: {detail['hybrid_score']:.3f}\")\n",
    "                if detail['header'] != 'N/A':\n",
    "                    print(f\"    Section: {detail['header']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "\n",
    "# Uncomment to run interactive demo\n",
    "# interactive_semantic_qa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices for Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunking_best_practices():\n",
    "    \"\"\"Display best practices for semantic chunking.\"\"\"\n",
    "    \n",
    "    practices = {\n",
    "        \"🎯 Strategy Selection\": [\n",
    "            \"• Use sentence-based for general text processing\",\n",
    "            \"• Use paragraph-based for well-structured documents\",\n",
    "            \"• Use structure-based for academic papers and reports\",\n",
    "            \"• Use embedding-based for maximum semantic coherence\",\n",
    "            \"• Consider hybrid approaches for complex documents\"\n",
    "        ],\n",
    "        \n",
    "        \"📏 Size Optimization\": [\n",
    "            \"• Balance semantic coherence with computational efficiency\",\n",
    "            \"• Allow variable chunk sizes within reasonable bounds\",\n",
    "            \"• Set minimum sizes to ensure meaningful content\",\n",
    "            \"• Consider context window limits of your target model\",\n",
    "            \"• Test different size parameters for your use case\"\n",
    "        ],\n",
    "        \n",
    "        \"🔗 Context Preservation\": [\n",
    "            \"• Maintain document structure information\",\n",
    "            \"• Preserve headers and section boundaries\",\n",
    "            \"• Include metadata about chunk relationships\",\n",
    "            \"• Consider hierarchical chunking for complex documents\",\n",
    "            \"• Track semantic similarity between adjacent chunks\"\n",
    "        ],\n",
    "        \n",
    "        \"⚡ Performance Considerations\": [\n",
    "            \"• Cache embeddings when processing multiple queries\",\n",
    "            \"• Use efficient embedding models for real-time applications\",\n",
    "            \"• Consider preprocessing documents in batches\",\n",
    "            \"• Optimize similarity thresholds based on your domain\",\n",
    "            \"• Monitor processing time vs. quality trade-offs\"\n",
    "        ],\n",
    "        \n",
    "        \"🔍 Quality Assurance\": [\n",
    "            \"• Validate chunk coherence with similarity metrics\",\n",
    "            \"• Test retrieval quality on representative queries\",\n",
    "            \"• Compare with simpler chunking methods as baselines\",\n",
    "            \"• Monitor for edge cases and boundary issues\",\n",
    "            \"• Regularly evaluate against your specific use case\"\n",
    "        ],\n",
    "        \n",
    "        \"🛠️ Implementation Tips\": [\n",
    "            \"• Start with paragraph-based chunking for most applications\",\n",
    "            \"• Use spaCy or NLTK for robust text preprocessing\",\n",
    "            \"• Implement fallback strategies for edge cases\",\n",
    "            \"• Consider domain-specific adaptations\",\n",
    "            \"• Document your chunking strategy and parameters\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"📚 Semantic Chunking Best Practices\\n\")\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"{category}\")\n",
    "        for tip in tips:\n",
    "            print(f\"  {tip}\")\n",
    "        print()\n",
    "\n",
    "semantic_chunking_best_practices()\n",
    "\n",
    "# Summary comparison table\n",
    "print(\"\\n📊 Semantic vs Fixed-Size Chunking Summary:\")\n",
    "comparison_table = pd.DataFrame([\n",
    "    {\n",
    "        'Aspect': 'Semantic Coherence',\n",
    "        'Semantic Chunking': 'High - preserves meaning',\n",
    "        'Fixed-Size Chunking': 'Variable - may break context'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Processing Speed',\n",
    "        'Semantic Chunking': 'Slower - requires NLP processing',\n",
    "        'Fixed-Size Chunking': 'Fast - simple token counting'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Chunk Size Consistency',\n",
    "        'Semantic Chunking': 'Variable - adapts to content',\n",
    "        'Fixed-Size Chunking': 'Consistent - predictable sizes'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Document Structure',\n",
    "        'Semantic Chunking': 'Preserves - respects boundaries',\n",
    "        'Fixed-Size Chunking': 'Ignores - arbitrary splits'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Implementation Complexity',\n",
    "        'Semantic Chunking': 'Complex - multiple strategies',\n",
    "        'Fixed-Size Chunking': 'Simple - straightforward'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Best Use Cases',\n",
    "        'Semantic Chunking': 'Q&A, analysis, comprehension',\n",
    "        'Fixed-Size Chunking': 'Batch processing, simple tasks'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Future Extensions\n",
    "\n",
    "In this notebook, we've explored semantic chunking techniques and built an intelligent document Q&A system using Google Gemini. \n",
    "\n",
    "### Key Achievements:\n",
    "1. **Implemented multiple semantic chunking strategies** preserving document structure and meaning\n",
    "2. **Built a sophisticated Q&A system** using semantic similarity and hybrid search\n",
    "3. **Compared semantic vs fixed-size approaches** with comprehensive metrics\n",
    "4. **Analyzed semantic coherence** and chunk quality across different methods\n",
    "5. **Demonstrated real-world applications** with complex research documents\n",
    "\n",
    "### Semantic Chunking Advantages:\n",
    "- **🧠 Semantic Coherence**: Preserves meaning and context boundaries\n",
    "- **📚 Structure Awareness**: Respects document organization and hierarchy\n",
    "- **🔍 Better Retrieval**: Improves relevance of retrieved content\n",
    "- **💡 Contextual Understanding**: Maintains logical flow and relationships\n",
    "- **🎯 Adaptive Sizing**: Adjusts chunk boundaries to content naturally\n",
    "\n",
    "### Next Steps:\n",
    "- **Hierarchical Chunking**: Implement multi-level document analysis\n",
    "- **Domain Adaptation**: Customize chunking for specific fields (legal, medical, technical)\n",
    "- **Dynamic Chunking**: Adapt strategies based on query types\n",
    "- **Multimodal Integration**: Handle documents with images, tables, and figures\n",
    "- **Real-time Processing**: Optimize for streaming and incremental updates\n",
    "\n",
    "### Production Considerations:\n",
    "- **Caching**: Store embeddings and processed chunks for efficiency\n",
    "- **Scalability**: Implement batch processing for large document collections\n",
    "- **Monitoring**: Track chunk quality and retrieval performance\n",
    "- **A/B Testing**: Compare chunking strategies for your specific use case\n",
    "- **User Feedback**: Incorporate user satisfaction metrics\n",
    "\n",
    "Semantic chunking represents a significant advance over fixed-size approaches, offering better context preservation and improved performance for understanding-based tasks. The choice of strategy should be guided by your specific requirements, document types, and performance constraints.\n",
    "\n",
    "**Happy semantic chunking!** 🚀🧠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
