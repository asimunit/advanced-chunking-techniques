{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping Chunks with Google Gemini\n",
    "## Building Context-Aware Document Processing Systems\n",
    "\n",
    "This notebook explores overlapping chunking techniques that preserve context across chunk boundaries, ensuring critical information isn't lost when processing large documents. We'll build an intelligent document analysis system using various overlap strategies with Google Gemini.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Understanding overlapping chunk principles and benefits\n",
    "- Implementing multiple overlap strategies (fixed, percentage, semantic)\n",
    "- Optimizing overlap size for different use cases\n",
    "- Building context-aware Q&A systems with Gemini\n",
    "- Analyzing information retention and redundancy\n",
    "- Performance optimization and trade-off analysis\n",
    "\n",
    "### Project Overview:\n",
    "We'll create an advanced system that:\n",
    "1. Implements various overlapping strategies for different document types\n",
    "2. Analyzes optimal overlap sizes and patterns\n",
    "3. Builds intelligent retrieval with overlap-aware ranking\n",
    "4. Demonstrates context preservation benefits\n",
    "5. Provides comprehensive performance analysis and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai sentence-transformers spacy nltk scikit-learn numpy pandas matplotlib seaborn tiktoken networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional dependencies\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API and models\n",
    "GEMINI_API_KEY = \"your-gemini-api-key-here\"  # Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize models\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"‚úÖ All models initialized successfully!\")\n",
    "print(f\"üìä Embedding dimensions: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"üß† spaCy model: {nlp.meta['name']} v{nlp.meta['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Overlapping Chunks\n",
    "\n",
    "Overlapping chunks ensure context preservation by sharing content between adjacent chunks, preventing information loss at boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def demonstrate_overlap_concept():\n",
    "    \"\"\"Demonstrate the concept and benefits of overlapping chunks.\"\"\"\n",
    "    \n",
    "    sample_text = \"\"\"\n",
    "    Artificial intelligence has revolutionized modern computing. Machine learning algorithms \n",
    "    can now process vast amounts of data to identify patterns. Deep learning networks, \n",
    "    inspired by the human brain, excel at complex tasks. Natural language processing \n",
    "    enables computers to understand human communication. Computer vision systems can \n",
    "    analyze images and videos with remarkable accuracy. These technologies are transforming \n",
    "    industries from healthcare to autonomous vehicles.\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    sentences = sent_tokenize(sample_text)\n",
    "    \n",
    "    print(\"üîç Overlap Concept Demonstration\\n\")\n",
    "    print(\"Original text:\")\n",
    "    print(sample_text)\n",
    "    print(f\"\\nTotal tokens: {count_tokens(sample_text)}\")\n",
    "    print(f\"Sentences: {len(sentences)}\\n\")\n",
    "    \n",
    "    # Show non-overlapping vs overlapping chunking\n",
    "    print(\"üìä Comparison: Non-overlapping vs Overlapping\\n\")\n",
    "    \n",
    "    # Non-overlapping chunks (2 sentences each)\n",
    "    print(\"üö´ Non-overlapping chunks:\")\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        chunk = ' '.join(sentences[i:i+2])\n",
    "        print(f\"Chunk {i//2 + 1}: {chunk}\")\n",
    "        print(f\"  Tokens: {count_tokens(chunk)}\\n\")\n",
    "    \n",
    "    # Overlapping chunks (2 sentences each, 1 sentence overlap)\n",
    "    print(\"‚úÖ Overlapping chunks (50% overlap):\")\n",
    "    for i in range(len(sentences) - 1):\n",
    "        chunk = ' '.join(sentences[i:i+2])\n",
    "        print(f\"Chunk {i + 1}: {chunk}\")\n",
    "        print(f\"  Tokens: {count_tokens(chunk)}\")\n",
    "        if i > 0:\n",
    "            overlap = sentences[i]\n",
    "            print(f\"  Overlap: '{overlap[:50]}...'\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate overlap statistics\n",
    "    total_unique_tokens = count_tokens(sample_text)\n",
    "    overlapping_total_tokens = sum(count_tokens(' '.join(sentences[i:i+2])) \n",
    "                                 for i in range(len(sentences) - 1))\n",
    "    \n",
    "    redundancy = (overlapping_total_tokens - total_unique_tokens) / total_unique_tokens * 100\n",
    "    \n",
    "    print(f\"üìà Overlap Statistics:\")\n",
    "    print(f\"  Unique content tokens: {total_unique_tokens}\")\n",
    "    print(f\"  Total tokens with overlap: {overlapping_total_tokens}\")\n",
    "    print(f\"  Redundancy: {redundancy:.1f}%\")\n",
    "    print(f\"  Context preservation: ‚úÖ Information bridged across boundaries\")\n",
    "\n",
    "demonstrate_overlap_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Overlapping Chunker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlappingChunker:\n",
    "    def __init__(self, chunk_size: int = 512, overlap_strategy: str = 'fixed', \n",
    "                 overlap_size: Union[int, float] = 50, min_chunk_size: int = 100):\n",
    "        \"\"\"\n",
    "        Advanced overlapping chunker with multiple strategies.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Target size of each chunk (in tokens)\n",
    "            overlap_strategy: 'fixed', 'percentage', 'sentence', 'semantic', 'adaptive'\n",
    "            overlap_size: Size of overlap (tokens for 'fixed', 0-1 for 'percentage')\n",
    "            min_chunk_size: Minimum acceptable chunk size\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_strategy = overlap_strategy\n",
    "        self.overlap_size = overlap_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text.\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def _extract_sentences(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Extract sentences with metadata.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = []\n",
    "        \n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            sent_text = sent.text.strip()\n",
    "            if sent_text:\n",
    "                sentences.append({\n",
    "                    'id': i,\n",
    "                    'text': sent_text,\n",
    "                    'start': sent.start_char,\n",
    "                    'end': sent.end_char,\n",
    "                    'tokens': self._count_tokens(sent_text),\n",
    "                    'words': len(sent_text.split())\n",
    "                })\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Main chunking method with overlap strategy.\"\"\"\n",
    "        # Clean and prepare text\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        sentences = self._extract_sentences(text)\n",
    "        \n",
    "        if not sentences:\n",
    "            return []\n",
    "        \n",
    "        if self.overlap_strategy == 'fixed':\n",
    "            return self._chunk_with_fixed_overlap(sentences)\n",
    "        elif self.overlap_strategy == 'percentage':\n",
    "            return self._chunk_with_percentage_overlap(sentences)\n",
    "        elif self.overlap_strategy == 'sentence':\n",
    "            return self._chunk_with_sentence_overlap(sentences)\n",
    "        else:\n",
    "            return self._chunk_with_percentage_overlap(sentences)  # Default\n",
    "    \n",
    "    def _chunk_with_fixed_overlap(self, sentences: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create chunks with fixed token overlap.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            sentence = sentences[i]\n",
    "            \n",
    "            # Check if adding this sentence exceeds chunk size\n",
    "            if (current_tokens + sentence['tokens'] > self.chunk_size and \n",
    "                current_tokens >= self.min_chunk_size):\n",
    "                \n",
    "                # Create chunk\n",
    "                chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "                overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "                \n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'sentences': current_chunk_sentences.copy(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'sentence_count': len(current_chunk_sentences),\n",
    "                    'strategy': self.overlap_strategy,\n",
    "                    'overlap_info': overlap_info\n",
    "                })\n",
    "                \n",
    "                # Calculate overlap for next chunk\n",
    "                overlap_tokens = min(int(self.overlap_size), current_tokens // 2)\n",
    "                \n",
    "                # Find sentences to include in overlap\n",
    "                overlap_sentences = []\n",
    "                overlap_token_count = 0\n",
    "                \n",
    "                for si in reversed(current_chunk_sentences):\n",
    "                    if overlap_token_count + sentences[si]['tokens'] <= overlap_tokens:\n",
    "                        overlap_sentences.insert(0, si)\n",
    "                        overlap_token_count += sentences[si]['tokens']\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # Start next chunk with overlap\n",
    "                if i + 1 < len(sentences):\n",
    "                    current_chunk_sentences = overlap_sentences + [i + 1]\n",
    "                    current_tokens = sum(sentences[si]['tokens'] for si in current_chunk_sentences)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk_sentences.append(i)\n",
    "                current_tokens += sentence['tokens']\n",
    "                i += 1\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_sentences and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "            overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "            \n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'sentences': current_chunk_sentences,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'strategy': self.overlap_strategy,\n",
    "                'overlap_info': overlap_info\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_with_percentage_overlap(self, sentences: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create chunks with percentage-based overlap.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            sentence = sentences[i]\n",
    "            \n",
    "            if (current_tokens + sentence['tokens'] > self.chunk_size and \n",
    "                current_tokens >= self.min_chunk_size):\n",
    "                \n",
    "                # Create chunk\n",
    "                chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "                overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "                \n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'sentences': current_chunk_sentences.copy(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'sentence_count': len(current_chunk_sentences),\n",
    "                    'strategy': self.overlap_strategy,\n",
    "                    'overlap_info': overlap_info\n",
    "                })\n",
    "                \n",
    "                # Calculate percentage-based overlap\n",
    "                overlap_tokens = int(current_tokens * self.overlap_size)\n",
    "                \n",
    "                # Find sentences for overlap\n",
    "                overlap_sentences = []\n",
    "                overlap_token_count = 0\n",
    "                \n",
    "                for si in reversed(current_chunk_sentences):\n",
    "                    if overlap_token_count + sentences[si]['tokens'] <= overlap_tokens:\n",
    "                        overlap_sentences.insert(0, si)\n",
    "                        overlap_token_count += sentences[si]['tokens']\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # Start next chunk\n",
    "                if i + 1 < len(sentences):\n",
    "                    current_chunk_sentences = overlap_sentences + [i + 1]\n",
    "                    current_tokens = sum(sentences[si]['tokens'] for si in current_chunk_sentences)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk_sentences.append(i)\n",
    "                current_tokens += sentence['tokens']\n",
    "                i += 1\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_sentences and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "            overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "            \n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'sentences': current_chunk_sentences,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'strategy': self.overlap_strategy,\n",
    "                'overlap_info': overlap_info\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_with_sentence_overlap(self, sentences: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create chunks with sentence-boundary overlap.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        overlap_sentences_count = int(self.overlap_size) if isinstance(self.overlap_size, (int, float)) else 1\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            sentence = sentences[i]\n",
    "            \n",
    "            if (current_tokens + sentence['tokens'] > self.chunk_size and \n",
    "                current_tokens >= self.min_chunk_size):\n",
    "                \n",
    "                # Create chunk\n",
    "                chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "                overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "                \n",
    "                chunks.append({\n",
    "                    'id': chunk_id,\n",
    "                    'text': chunk_text,\n",
    "                    'sentences': current_chunk_sentences.copy(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'sentence_count': len(current_chunk_sentences),\n",
    "                    'strategy': self.overlap_strategy,\n",
    "                    'overlap_info': overlap_info\n",
    "                })\n",
    "                \n",
    "                # Take last N sentences for overlap\n",
    "                overlap_start = max(0, len(current_chunk_sentences) - overlap_sentences_count)\n",
    "                overlap_sentences = current_chunk_sentences[overlap_start:]\n",
    "                \n",
    "                # Start next chunk\n",
    "                current_chunk_sentences = overlap_sentences + [i]\n",
    "                current_tokens = sum(sentences[si]['tokens'] for si in current_chunk_sentences)\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                current_chunk_sentences.append(i)\n",
    "                current_tokens += sentence['tokens']\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk_sentences and current_tokens >= self.min_chunk_size:\n",
    "            chunk_text = ' '.join([sentences[si]['text'] for si in current_chunk_sentences])\n",
    "            overlap_info = self._calculate_overlap_info(chunks, current_chunk_sentences, sentences)\n",
    "            \n",
    "            chunks.append({\n",
    "                'id': chunk_id,\n",
    "                'text': chunk_text,\n",
    "                'sentences': current_chunk_sentences,\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_chunk_sentences),\n",
    "                'strategy': self.overlap_strategy,\n",
    "                'overlap_info': overlap_info\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _calculate_overlap_info(self, existing_chunks: List[Dict], \n",
    "                              current_sentences: List[int], \n",
    "                              all_sentences: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate overlap information for a chunk.\"\"\"\n",
    "        if not existing_chunks:\n",
    "            return {'overlap_with_previous': 0, 'overlap_sentences': [], 'overlap_tokens': 0}\n",
    "        \n",
    "        previous_chunk = existing_chunks[-1]\n",
    "        previous_sentences = set(previous_chunk['sentences'])\n",
    "        current_sentences_set = set(current_sentences)\n",
    "        \n",
    "        overlap_sentences = list(previous_sentences & current_sentences_set)\n",
    "        overlap_tokens = sum(all_sentences[si]['tokens'] for si in overlap_sentences)\n",
    "        \n",
    "        return {\n",
    "            'overlap_with_previous': len(overlap_sentences),\n",
    "            'overlap_sentences': overlap_sentences,\n",
    "            'overlap_tokens': overlap_tokens\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ OverlappingChunker class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Different Overlap Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document for overlap analysis\n",
    "test_document = \"\"\"\n",
    "Introduction to Quantum Computing\n",
    "\n",
    "Quantum computing represents a revolutionary approach to information processing that leverages quantum mechanical phenomena. Unlike classical computers that use bits representing either 0 or 1, quantum computers use quantum bits or qubits that can exist in superposition states.\n",
    "\n",
    "The principle of superposition allows qubits to be in multiple states simultaneously. This fundamental property enables quantum computers to perform certain calculations exponentially faster than classical computers. Quantum entanglement is another crucial phenomenon where qubits become correlated in ways that classical physics cannot explain.\n",
    "\n",
    "Historical Development\n",
    "\n",
    "The theoretical foundations of quantum computing were laid in the 1980s by physicists like Richard Feynman and David Deutsch. Feynman proposed that quantum systems could be used to simulate other quantum systems more efficiently than classical computers.\n",
    "\n",
    "The first quantum algorithms were developed in the 1990s. Peter Shor's algorithm for factoring large numbers demonstrated quantum computing's potential to break current cryptographic systems. Lov Grover's search algorithm showed quadratic speedup for searching unsorted databases.\n",
    "\n",
    "Quantum Algorithms and Applications\n",
    "\n",
    "Shor's algorithm poses a significant threat to RSA encryption, which relies on the difficulty of factoring large numbers. A sufficiently large quantum computer could break RSA encryption in polynomial time, rendering current security systems vulnerable.\n",
    "\n",
    "Grover's algorithm provides quadratic speedup for searching unstructured databases. While classical computers require O(N) operations to search N items, Grover's algorithm requires only O(‚àöN) operations, offering substantial performance improvements.\n",
    "\n",
    "Quantum machine learning algorithms show promise for optimization problems and pattern recognition. Variational quantum eigensolvers can solve chemistry problems by finding ground state energies of molecules.\n",
    "\"\"\"\n",
    "\n",
    "# Test different overlap strategies\n",
    "overlap_strategies = {\n",
    "    'Fixed (50 tokens)': OverlappingChunker(chunk_size=300, overlap_strategy='fixed', overlap_size=50),\n",
    "    'Percentage (20%)': OverlappingChunker(chunk_size=300, overlap_strategy='percentage', overlap_size=0.2),\n",
    "    'Sentence (2 sentences)': OverlappingChunker(chunk_size=300, overlap_strategy='sentence', overlap_size=2),\n",
    "    'No Overlap': OverlappingChunker(chunk_size=300, overlap_strategy='fixed', overlap_size=0)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"üî¨ Testing Overlapping Chunk Strategies\\n\")\n",
    "\n",
    "for name, chunker in overlap_strategies.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Strategy: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    chunks = chunker.chunk_text(test_document)\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate overlap statistics\n",
    "    total_tokens = sum(chunk['token_count'] for chunk in chunks)\n",
    "    unique_tokens = count_tokens(test_document)\n",
    "    overlap_tokens = sum(chunk['overlap_info']['overlap_tokens'] for chunk in chunks)\n",
    "    redundancy = (total_tokens - unique_tokens) / unique_tokens * 100 if unique_tokens > 0 else 0\n",
    "    \n",
    "    results[name] = {\n",
    "        'chunks': chunks,\n",
    "        'count': len(chunks),\n",
    "        'avg_tokens': np.mean([c['token_count'] for c in chunks]),\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': unique_tokens,\n",
    "        'overlap_tokens': overlap_tokens,\n",
    "        'redundancy': redundancy,\n",
    "        'processing_time': processing_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Average tokens per chunk: {np.mean([c['token_count'] for c in chunks]):.1f}\")\n",
    "    print(f\"Total tokens (with overlap): {total_tokens}\")\n",
    "    print(f\"Unique content tokens: {unique_tokens}\")\n",
    "    print(f\"Overlap tokens: {overlap_tokens}\")\n",
    "    print(f\"Redundancy: {redundancy:.1f}%\")\n",
    "    print(f\"Processing time: {processing_time:.3f}s\")\n",
    "    \n",
    "    # Show overlap pattern for first few chunks\n",
    "    print(f\"\\nüìä Overlap Pattern:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        overlap_info = chunk['overlap_info']\n",
    "        print(f\"  Chunk {i}: {overlap_info['overlap_with_previous']} overlapping sentences, \"\n",
    "              f\"{overlap_info['overlap_tokens']} tokens\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Overlap Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "strategies = list(results.keys())\n",
    "chunk_counts = [results[s]['count'] for s in strategies]\n",
    "avg_tokens = [results[s]['avg_tokens'] for s in strategies]\n",
    "redundancy = [results[s]['redundancy'] for s in strategies]\n",
    "proc_times = [results[s]['processing_time'] for s in strategies]\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Overlap Strategy Comparison', fontsize=16)\n",
    "\n",
    "# Chunk count comparison\n",
    "ax1.bar(strategies, chunk_counts, alpha=0.7, color='skyblue')\n",
    "ax1.set_title('Number of Chunks')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Average tokens per chunk\n",
    "ax2.bar(strategies, avg_tokens, alpha=0.7, color='lightgreen')\n",
    "ax2.set_title('Average Tokens per Chunk')\n",
    "ax2.set_ylabel('Tokens')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Redundancy percentage\n",
    "ax3.bar(strategies, redundancy, alpha=0.7, color='orange')\n",
    "ax3.set_title('Context Redundancy')\n",
    "ax3.set_ylabel('Percentage (%)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing time\n",
    "ax4.bar(strategies, proc_times, alpha=0.7, color='lightcoral')\n",
    "ax4.set_title('Processing Time')\n",
    "ax4.set_ylabel('Seconds')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': strategy,\n",
    "        'Chunks': data['count'],\n",
    "        'Avg Tokens': f\"{data['avg_tokens']:.1f}\",\n",
    "        'Total Tokens': data['total_tokens'],\n",
    "        'Redundancy %': f\"{data['redundancy']:.1f}\",\n",
    "        'Processing Time (s)': f\"{data['processing_time']:.3f}\"\n",
    "    }\n",
    "    for strategy, data in results.items()\n",
    "])\n",
    "\n",
    "print(\"\\nüìä Strategy Comparison Results:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "print(\"‚Ä¢ Overlapping strategies provide more context but increase redundancy\")\n",
    "print(\"‚Ä¢ Percentage-based overlap adapts to chunk content naturally\")\n",
    "print(\"‚Ä¢ Sentence-based overlap preserves natural language boundaries\")\n",
    "print(\"‚Ä¢ Higher redundancy may improve context preservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overlap-Aware Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapAwareQASystem:\n",
    "    def __init__(self, overlap_strategy: str = 'percentage', chunk_size: int = 400, \n",
    "                 overlap_size: Union[int, float] = 0.2):\n",
    "        self.chunker = OverlappingChunker(\n",
    "            chunk_size=chunk_size, \n",
    "            overlap_strategy=overlap_strategy, \n",
    "            overlap_size=overlap_size\n",
    "        )\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.chunks = []\n",
    "        self.chunk_embeddings = None\n",
    "        self.document_title = \"\"\n",
    "        \n",
    "    def load_document(self, text: str, title: str = \"Document\"):\n",
    "        \"\"\"Load document and create overlap-aware chunks.\"\"\"\n",
    "        self.document_title = title\n",
    "        print(f\"üîÑ Processing document with {self.chunker.overlap_strategy} overlap strategy...\")\n",
    "        \n",
    "        # Create overlapping chunks\n",
    "        self.chunks = self.chunker.chunk_text(text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        chunk_texts = [chunk['text'] for chunk in self.chunks]\n",
    "        self.chunk_embeddings = self.embedding_model.encode(chunk_texts)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_tokens = sum(chunk['token_count'] for chunk in self.chunks)\n",
    "        unique_tokens = count_tokens(text)\n",
    "        redundancy = (total_tokens - unique_tokens) / unique_tokens * 100\n",
    "        \n",
    "        print(f\"‚úÖ Loaded '{title}' with {len(self.chunks)} overlapping chunks\")\n",
    "        print(f\"üìä Redundancy: {redundancy:.1f}% | Strategy: {self.chunker.overlap_strategy}\")\n",
    "        \n",
    "    def _find_relevant_chunks(self, question: str, max_chunks: int = 3) -> List[Dict]:\n",
    "        \"\"\"Find relevant chunks using semantic similarity.\"\"\"\n",
    "        if not self.chunks or self.chunk_embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Get question embedding and similarities\n",
    "        question_embedding = self.embedding_model.encode([question])\n",
    "        similarities = cosine_similarity(question_embedding, self.chunk_embeddings)[0]\n",
    "        \n",
    "        # Get top chunks by similarity\n",
    "        top_indices = np.argsort(similarities)[::-1][:max_chunks]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            chunk['similarity_score'] = float(similarities[idx])\n",
    "            relevant_chunks.append(chunk)\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict:\n",
    "        \"\"\"Answer question using overlap-aware retrieval.\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        print(f\"üîç Finding relevant chunks with overlap awareness: {question}\")\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        relevant_chunks = self._find_relevant_chunks(question, max_chunks=3)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\"error\": \"No relevant content found\"}\n",
    "        \n",
    "        # Sort chunks by document order for coherent context\n",
    "        relevant_chunks.sort(key=lambda x: x['id'])\n",
    "        \n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        for chunk in relevant_chunks:\n",
    "            chunk_info = f\"[Chunk {chunk['id']}]\"\n",
    "            context_parts.append(f\"{chunk_info}\\n{chunk['text']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer_prompt = f\"\"\"\n",
    "        You are analyzing a document with overlapping chunks to ensure context continuity. \n",
    "        Answer the question based on the provided context from \"{self.document_title}\".\n",
    "        \n",
    "        Context (chunks may have overlapping content for continuity):\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Instructions:\n",
    "        1. Provide a comprehensive answer using information from all relevant chunks\n",
    "        2. When chunks overlap, synthesize the information without redundancy\n",
    "        3. Use the overlapping context to provide smoother, more connected answers\n",
    "        4. Cite which chunks your information comes from when helpful\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.gemini_model.generate_content(answer_prompt)\n",
    "            \n",
    "            # Calculate overlap statistics for this retrieval\n",
    "            total_context_tokens = sum(chunk['token_count'] for chunk in relevant_chunks)\n",
    "            total_overlap_tokens = sum(chunk['overlap_info']['overlap_tokens'] for chunk in relevant_chunks)\n",
    "            context_redundancy = (total_overlap_tokens / total_context_tokens * 100) if total_context_tokens > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.text,\n",
    "                \"relevant_chunks\": len(relevant_chunks),\n",
    "                \"chunk_details\": [\n",
    "                    {\n",
    "                        \"id\": chunk['id'],\n",
    "                        \"similarity\": chunk.get('similarity_score', 0),\n",
    "                        \"token_count\": chunk['token_count'],\n",
    "                        \"overlap_tokens\": chunk['overlap_info']['overlap_tokens']\n",
    "                    }\n",
    "                    for chunk in relevant_chunks\n",
    "                ],\n",
    "                \"context_tokens\": total_context_tokens,\n",
    "                \"context_redundancy\": context_redundancy,\n",
    "                \"overlap_strategy\": self.chunker.overlap_strategy\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to generate answer: {e}\"}\n",
    "    \n",
    "    def analyze_overlap_structure(self) -> Dict:\n",
    "        \"\"\"Analyze the overlap structure of the document.\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"error\": \"No document loaded\"}\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"overlap_strategy\": self.chunker.overlap_strategy,\n",
    "            \"avg_tokens_per_chunk\": np.mean([c['token_count'] for c in self.chunks]),\n",
    "            \"overlap_statistics\": {\n",
    "                \"avg_overlap_tokens\": np.mean([c['overlap_info']['overlap_tokens'] for c in self.chunks if c['overlap_info']['overlap_tokens'] > 0]),\n",
    "                \"max_overlap_tokens\": max([c['overlap_info']['overlap_tokens'] for c in self.chunks]),\n",
    "                \"total_overlap_tokens\": sum([c['overlap_info']['overlap_tokens'] for c in self.chunks])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"‚úÖ OverlapAwareQASystem class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the Overlap-Aware Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize overlap-aware Q&A system\n",
    "overlap_qa = OverlapAwareQASystem(\n",
    "    overlap_strategy='percentage', \n",
    "    chunk_size=400, \n",
    "    overlap_size=0.25  # 25% overlap\n",
    ")\n",
    "\n",
    "overlap_qa.load_document(test_document, \"Quantum Computing Introduction\")\n",
    "\n",
    "# Analyze overlap structure\n",
    "overlap_analysis = overlap_qa.analyze_overlap_structure()\n",
    "print(f\"\\nüìä Overlap Structure Analysis:\")\n",
    "for key, value in overlap_analysis.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"    {subkey}: {subvalue}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is quantum superposition and how does it provide computational advantages?\",\n",
    "    \"Explain Shor's algorithm and its impact on cryptography\",\n",
    "    \"What are the key differences between quantum and classical computing?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß† Testing Overlap-Aware Q&A System\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions[:2], 1):  # Test first 2 questions\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = overlap_qa.answer_question(question)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"\\nüí° Answer:\")\n",
    "        print(result[\"answer\"])\n",
    "        \n",
    "        print(f\"\\nüìä Retrieval Statistics:\")\n",
    "        print(f\"  - Overlap strategy: {result['overlap_strategy']}\")\n",
    "        print(f\"  - Relevant chunks: {result['relevant_chunks']}\")\n",
    "        print(f\"  - Context tokens: {result['context_tokens']}\")\n",
    "        print(f\"  - Context redundancy: {result['context_redundancy']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìö Chunk Usage Details:\")\n",
    "        for detail in result['chunk_details']:\n",
    "            print(f\"  ‚Ä¢ Chunk {detail['id']}: Similarity {detail['similarity']:.3f}, \"\n",
    "                  f\"Tokens {detail['token_count']}, Overlap {detail['overlap_tokens']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Overlapping Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_chunks_best_practices():\n",
    "    \"\"\"Display comprehensive best practices for overlapping chunk strategies.\"\"\"\n",
    "    \n",
    "    practices = {\n",
    "        \"üéØ Overlap Strategy Selection\": [\n",
    "            \"‚Ä¢ Use fixed overlap (50-100 tokens) for consistent, predictable redundancy\",\n",
    "            \"‚Ä¢ Use percentage overlap (15-25%) for adaptive redundancy based on chunk size\",\n",
    "            \"‚Ä¢ Use sentence overlap (1-2 sentences) for natural language boundaries\",\n",
    "            \"‚Ä¢ Consider document structure when choosing overlap strategy\",\n",
    "            \"‚Ä¢ Test multiple strategies with your specific content type\"\n",
    "        ],\n",
    "        \n",
    "        \"üìè Optimal Overlap Sizing\": [\n",
    "            \"‚Ä¢ Start with 15-25% overlap for most applications\",\n",
    "            \"‚Ä¢ Use smaller overlaps (10-15%) for highly structured documents\",\n",
    "            \"‚Ä¢ Use larger overlaps (25-40%) for narrative or complex content\",\n",
    "            \"‚Ä¢ Balance context preservation with computational efficiency\",\n",
    "            \"‚Ä¢ Monitor redundancy vs. performance trade-offs\"\n",
    "        ],\n",
    "        \n",
    "        \"üîó Context Preservation Techniques\": [\n",
    "            \"‚Ä¢ Preserve sentence boundaries whenever possible\",\n",
    "            \"‚Ä¢ Consider semantic similarity when determining overlap content\",\n",
    "            \"‚Ä¢ Track overlap tokens to avoid excessive redundancy\",\n",
    "            \"‚Ä¢ Use overlap for bridging concepts across chunk boundaries\",\n",
    "            \"‚Ä¢ Implement quality metrics for context preservation\"\n",
    "        ],\n",
    "        \n",
    "        \"‚ö° Performance Optimization\": [\n",
    "            \"‚Ä¢ Cache overlap calculations for repeated processing\",\n",
    "            \"‚Ä¢ Use efficient similarity search to handle redundancy\",\n",
    "            \"‚Ä¢ Monitor memory usage with large overlap percentages\",\n",
    "            \"‚Ä¢ Consider streaming processing for very large documents\",\n",
    "            \"‚Ä¢ Implement parallel processing where possible\"\n",
    "        ],\n",
    "        \n",
    "        \"üîç Quality Assurance\": [\n",
    "            \"‚Ä¢ Measure context preservation effectiveness\",\n",
    "            \"‚Ä¢ Test boundary scenarios and edge cases\",\n",
    "            \"‚Ä¢ Validate that overlaps actually improve retrieval quality\",\n",
    "            \"‚Ä¢ Monitor for information loss at chunk boundaries\",\n",
    "            \"‚Ä¢ Compare against non-overlapping baselines\"\n",
    "        ],\n",
    "        \n",
    "        \"‚ö†Ô∏è Common Pitfalls to Avoid\": [\n",
    "            \"‚Ä¢ Don't use excessive overlap (>50%) without strong justification\",\n",
    "            \"‚Ä¢ Avoid ignoring computational costs of high redundancy\",\n",
    "            \"‚Ä¢ Don't assume more overlap always means better performance\",\n",
    "            \"‚Ä¢ Avoid fixed strategies for variable content types\",\n",
    "            \"‚Ä¢ Don't neglect to measure actual improvement from overlaps\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üìö Overlapping Chunks Best Practices\\n\")\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"{category}\")\n",
    "        for tip in tips:\n",
    "            print(f\"  {tip}\")\n",
    "        print()\n",
    "\n",
    "overlapping_chunks_best_practices()\n",
    "\n",
    "# Create decision matrix for overlap strategy selection\n",
    "decision_matrix = pd.DataFrame([\n",
    "    {\n",
    "        'Document Type': 'Academic Papers',\n",
    "        'Recommended Strategy': 'Sentence (1-2)',\n",
    "        'Overlap Size': '15-25%',\n",
    "        'Reasoning': 'Preserve logical flow, respect paragraph structure'\n",
    "    },\n",
    "    {\n",
    "        'Document Type': 'Technical Manuals',\n",
    "        'Recommended Strategy': 'Fixed (50-75 tokens)',\n",
    "        'Overlap Size': '10-20%',\n",
    "        'Reasoning': 'Consistent structure, preserve procedural steps'\n",
    "    },\n",
    "    {\n",
    "        'Document Type': 'Narrative Content',\n",
    "        'Recommended Strategy': 'Percentage (20-30%)',\n",
    "        'Overlap Size': '20-35%',\n",
    "        'Reasoning': 'Maintain story flow, preserve character/plot context'\n",
    "    },\n",
    "    {\n",
    "        'Document Type': 'Legal Documents',\n",
    "        'Recommended Strategy': 'Sentence (2-3)',\n",
    "        'Overlap Size': '25-40%',\n",
    "        'Reasoning': 'Preserve legal context, maintain clause relationships'\n",
    "    },\n",
    "    {\n",
    "        'Document Type': 'News Articles',\n",
    "        'Recommended Strategy': 'Percentage (15-20%)',\n",
    "        'Overlap Size': '15-25%',\n",
    "        'Reasoning': 'Balance context with information density'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nüóÇÔ∏è Document Type Decision Matrix:\")\n",
    "print(decision_matrix.to_string(index=False))\n",
    "\n",
    "# Performance comparison summary\n",
    "performance_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Aspect': 'Context Preservation',\n",
    "        'No Overlap': '‚≠ê‚≠ê',\n",
    "        'Low Overlap (10-15%)': '‚≠ê‚≠ê‚≠ê',\n",
    "        'Medium Overlap (20-30%)': '‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "        'High Overlap (35%+)': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Computational Efficiency',\n",
    "        'No Overlap': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "        'Low Overlap (10-15%)': '‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "        'Medium Overlap (20-30%)': '‚≠ê‚≠ê‚≠ê',\n",
    "        'High Overlap (35%+)': '‚≠ê‚≠ê'\n",
    "    },\n",
    "    {\n",
    "        'Aspect': 'Answer Quality',\n",
    "        'No Overlap': '‚≠ê‚≠ê‚≠ê',\n",
    "        'Low Overlap (10-15%)': '‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "        'Medium Overlap (20-30%)': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "        'High Overlap (35%+)': '‚≠ê‚≠ê‚≠ê‚≠ê'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Performance Trade-off Matrix:\")\n",
    "print(performance_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Key Takeaways\n",
    "\n",
    "In this comprehensive notebook, we've explored overlapping chunk techniques and built sophisticated document processing systems using Google Gemini.\n",
    "\n",
    "### üéØ Key Achievements:\n",
    "1. **Implemented Multiple Overlap Strategies** - Fixed, percentage, and sentence-based overlapping\n",
    "2. **Built Overlap-Aware Q&A System** - Context-preserving retrieval capabilities\n",
    "3. **Analyzed Performance Trade-offs** - Redundancy vs. context quality balance\n",
    "4. **Developed Best Practices** - Guidelines for different document types\n",
    "5. **Created Comprehensive Comparisons** - Detailed analysis of strategy effectiveness\n",
    "\n",
    "### üöÄ Overlapping Chunks Advantages:\n",
    "- **üîó Context Continuity**: Preserves information across chunk boundaries\n",
    "- **üìà Improved Retrieval**: Better chance of capturing complete concepts\n",
    "- **üß† Enhanced Understanding**: Maintains narrative and logical flow\n",
    "- **‚ö° Flexible Strategies**: Adaptable to different document types and use cases\n",
    "- **üîç Better Q&A Quality**: More comprehensive and coherent answers\n",
    "\n",
    "### üìä Key Insights:\n",
    "- **Optimal Overlap Range**: 15-25% provides best balance for most applications\n",
    "- **Strategy Selection**: Document structure should guide overlap strategy choice\n",
    "- **Redundancy Trade-off**: Higher overlap improves context but increases computational cost\n",
    "- **Quality vs. Efficiency**: Balance between answer quality and processing resources\n",
    "\n",
    "### üõ†Ô∏è Production Considerations:\n",
    "- **Caching Strategy**: Store overlap calculations efficiently\n",
    "- **Scalability**: Implement distributed processing for large collections\n",
    "- **Monitoring**: Track overlap effectiveness and computational costs\n",
    "- **User Experience**: Balance answer quality with response time\n",
    "\n",
    "### üéì Key Takeaways:\n",
    "1. **Context is King**: Overlapping chunks significantly improve context preservation\n",
    "2. **One Size Doesn't Fit All**: Different documents require different overlap strategies\n",
    "3. **Trade-offs are Real**: Balance context quality with computational efficiency\n",
    "4. **Measurement Matters**: Always validate that overlaps improve actual performance\n",
    "5. **Strategy Flexibility**: Adapt overlap approach based on specific use cases\n",
    "\n",
    "Overlapping chunks represent a sophisticated approach to document processing that significantly improves context preservation and answer quality. The techniques demonstrated provide a solid foundation for building production-ready systems.\n",
    "\n",
    "**Happy overlapping!** üöÄüîó"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
